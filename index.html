<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Neel P. Bhatt</title>

  <meta name="description" content="Neel P. Bhatt's personal website" />
  <meta name="author" content="Neel P. Bhatt">
  <meta name="keywords" content="Neel P. Bhatt, Neel Bhatt, Autonomous Driving, Robotics, Computer Vision, AI, ML, Austin, Canada" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Latest compiled and minified CSS -->
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->

  <!-- jQuery library -->
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script> -->

  <!-- Latest compiled JavaScript -->
  <!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/profile_pic_v2.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CHJT8YCW79"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CHJT8YCW79');
</script>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b>Neel</b> P. Bhatt</name>
              </p>
              <p>I am currently a <a href="https://autonomy.oden.utexas.edu/profiles/neel-bhatt" target="_blank">Postdoctoral Fellow</a> at the <a href="https://autonomy.oden.utexas.edu/" target="_blank">Center for Autonomy</a> working with Professors <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a> and <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>. I am part of the <a href="https://autonomy.oden.utexas.edu/Groups/autonomous-systems-group" target="_blank">Autonomous Systems Group</a> and the <a href="https://vita-group.github.io/" target="_blank">VITA Research Group</a> at the University of Texas at Austin. I am developing neuro-symbolic architectures with research centered at the intersection of generative AI, assured active perception, prediction, and trustworthy sequential decision making for autonomous systems.</p>
              <p>I received my PhD in Mechatronics Engineering from University of Waterloo in 2023. During my PhD, I led efforts on the <a href="https://uwaterloo.ca/watonobus/" target="_blank">WATonoBus project</a> at <a href="https://uwaterloo.ca/mechatronic-vehicle-systems-lab/" target="_blank">MVS Lab</a> working on software and algorithmic development of perception and prediction modules required for <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">Canada’s first autonomous shuttle bus</a> approved via the ministry’s autonomous vehicle pilot. During this time, I also interned at <a href="https://www.gm.com/company/facilities/global-technical-center" target="_blank">GM R&D</a> where I worked on deep learning-based state estimation.</p>
              <p>I have also concurrently been a visiting research scholar since 2021 at the <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home" target="_blank">NODE Lab</a> at University of Alberta working on NODE lab's autonomous vehicle.</p>
              <p> Prior to this, I received my Bachelor's degree in Mechanical Engineering (I was directly admitted to a PhD from Bachelor's) in 2018 with focus on Mechatronics and Robotics from University of Toronto. During this time, I conducted research with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/" target="_blank">Yu Sun</a> on micro and nano robotics and interned at <a href="https://clearpathrobotics.com/" target="_blank">Clearpath Robotics</a> and was awarded <a href="https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp" target="_blank">NSERC Research Awards</a> for both.</p>
              <p style="text-align:center">
                    [
                <a href="mailto:npbhatt@utexas.edu" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/outlook_icon.png"> Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=S8ofWDUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/scholar_icon.png"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/neelbhattportfolio/" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/linkedin_icon.png"> LinkedIn</a> &nbsp/&nbsp
                <a href="cv/CV_Neel_Bhatt_July_2024.pdf" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/cv_logo.png"> CV</a>
]
              </p>
            </td>
            <td style="padding:1.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_v2.png" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_v2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                <li><span class="label label-success"><b>Jun 2024</b></span> <b>Paper Accepted @ IROS 2024</b> - Our work on <a href="https://arxiv.org/pdf/2404.00923" target="_blank">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements</a> was accepted at IROS 2024. <b style="color:red">Oral Pitch Finalist!</b></li>
                <li><span class="label label-success"><b>May 2024</b></span> <b>Talk @ DESTION Workshop and HP AI Labs</b> - I will be giving a talk at <a href="https://cps-vo.org/group/DESTION2024/program" target="_blank">DESTION workshop</a> and HP AI Labs on our fine-tuning work.</li>
                <li><span class="label label-success"><b>May 2024</b></span> <b>Serving as Associate Editor @ ITSC 2024</b> - I will be serving as an associate editor for <a href="https://ieee-itsc.org/2024/" target="_blank">ITSC 2024</a> [<a href="https://www.youtube.com/watch?v=SjlhfRUpOvA&ab_channel=NeelBhatt/" target="_blank">Video</a>].</li>
                <li><span class="label label-success"><b>Apr 2024</b></span> <b>Talk @ CMU</b> - I gave a talk on assured compositional RL for area and route search missions at the <a href="https://www.darpa.mil/program/assured-neuro-symbolic-learning-and-reasoning" target="_blank">DARPA ANSR</a> PI meeting #2 at CMU.</li>
                <li><span class="label label-success"><b>Mar 2024</b></span> <b>Paper Preprint</b> - Our paper on <a href="https://arxiv.org/pdf/2403.16993" target="_blank">Comp4D: LLM-Guided Compositional 4D Scene Generation</a> is up!</li>
                <li><span class="label label-success"><b>Feb 2024</b></span> <b>Paper Accepted @ MLSys 2024</b> - Our work on <a href="https://arxiv.org/pdf/2310.18239" target="_blank">Fine-tuning Language Models Using Formal Methods Feedback: A Use Case in Autonomous Systems</a> was accepted at: <a href="https://mlsys.org/virtual/2024/session/2779#:~:text=Fine%2DTuning%20Language%20Models%20Using%20Formal%20Methods%20Feedback%3A%20A%20Use%20Case%20in%20Autonomous%20Systems" target="_blank">MLSys 2024. <b style="color:red">One of only 37 accepted papers!</b></a></li>
                <li><span class="label label-success"><b>Jan 2024</b></span> <b>Talk @ Oxford</b> - I gave a talk at Oxford on our fine-tuning work.</li>
                <li><span class="label label-success"><b>Dec 2023</b></span> <b>Paper Accepted @ AAAI 2024</b> - Our work on <a href="https://arxiv.org/pdf/2310.18239" target="_blank">Fine-tuning Language Models Using Formal Methods Feedback was accepted at AAAI 2024: <a href="https://nuclear-workshop.github.io/" target="_blank">Neuro-Symbolic Learning and Reasoning in the Era of Large Language Models!</a></li>
                <li><span class="label label-success"><b>Oct 2023</b></span> <b>Talk @ UC Berkeley</b> - I gave a talk on my work at the <a href="https://www.darpa.mil/program/assured-neuro-symbolic-learning-and-reasoning" target="_blank">DARPA ANSR</a> PI meeting #1 at UC Berkeley.</li>
                <li><span class="label label-success"><b>Sep 2023</b></span> <b>Organizing Committee</b> - I am part of the organizing committee for <a href="https://ieee-itsc.org/2024/" target="_blank">ITSC 2024</a>.</li>
                <li><span class="label label-success"><b>Sep 2023</b></span> <b style="color:red">Started as a Postdoc</b> - I started as a Postdoctoral Fellow at <a href="https://oden.utexas.edu/" target="_blank">The University of Texas at Austin</a>!</li>
                <li><span class="label label-success"><b>Aug 2023</b></span> <b>Paper Accepted @ ITSC 2023</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">A Stereo Visual Odometry Framework with Augmented Perception for Dynamic Urban Environments</a> was accepted to <a href="https://2023.ieee-itsc.org/" target="_blank">ITSC 2023</a>.</li>
                <li><span class="label label-success"><b>Jul 2023</b></span> <b>Paper Published @ T-IV 2023</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">Integrated Inertial-LiDAR-Based Map Matching Localization for Varying Environments</a> was published in T-IV 2023.</li>
                <li><span class="label label-success"><b>Jun 2023</b></span> <b>Featured Video</b> - I was featured on the <a href="https://www.ualberta.ca/research/our-research/artificial-intelligence.html" target="_blank">homepage</a> of Artificial Intelligence Research and Innovation at University of Alberta via a <a href="https://youtu.be/bnfe1BZZiaw?t=52" target="_blank">promo video</a>. I also discussed about how I use AI in my research via this U of A <a href="https://www.linkedin.com/posts/university-of-alberta_ai-innovator-neel-bhatt-activity-7084575724824629248-NH1-?utm_source=share&utm_medium=member_desktop" target="_blank">video</a> and <a href="https://www.ualberta.ca/the-quad/2023/06/innovator-spotlight-neel-bhatt.html" target="_blank">article</a>.</li>
                <li><span class="label label-success"><b>Jun 2023</b></span> <b>Workshop @ IV 2023</b> - Co-organizing a workshop on <a href="https://sites.google.com/view/uofatutorialiv23/home" target="_blank">Reliable State Estimation and Distributed Controls in Intelligent Vehicular Networks</a> at <a href="https://2023.ieee-iv.org/" target="_blank">IV 2023</a>.</li>
                <li><span class="label label-success"><b>May 2023</b></span> <b style="color:red">Defended PhD Thesis</b> - My thesis work is out on <a href="https://uwspace.uwaterloo.ca/handle/10012/19210" target="_blank">UW Space</a>.</li> 
                <li><span class="label label-success"><b>May 2023</b></span> <b>Paper Published @ T-ITS 2023</b> - Our work <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</a> was accepted to T-ITS 2023.</li> 
                <li><span class="label label-success"><b>Apr 2023</b></span> <b>Guest Lecture</b> - Gave a guest lecture on WATonoBus ‑ Algorithms and Software Structure for an All Weather Shuttle for ECE495 at University of Waterloo.</li> 
                <li><span class="label label-success"><b>Oct 2022</b></span> <b>Paper Published @ IROS 2022</b> - I will be presenting our work <a href="https://sites.google.com/view/neverendingrl/home?authuser=0" target="_blank">MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</a> at IROS 2022 in Kyoto.</li>
                <li><span class="label label-success"><b>Aug 2022</b></span> <b>Paper Published @ CASE 2022</b> - Our work on <a href="https://www.mdpi.com/1782890" target="_blank">Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</a> was accepted to CASE 2022.</li>
                <li><span class="label label-success"><b>Aug 2022</b></span> <b>Paper Published @ Robotics 2022</b> - Our work on <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</a> was accepted to Robotics 2022.</li>
                <li><span class="label label-success"><b>Jun 2021</b></span> <b>WATonoBus Project</b> - The <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">autonomous shuttle project</a> I am leading at MVS Lab was given approval as part of the ministry's autonomous vehicle pilot.</li>
                <li><span class="label label-success"><b>Sep 2020</b></span> <b>Paper Published @ ITSC 2020</b> - I will be presenting our work <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">Real‑time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</a> at ITSC 2020 in Greece.</li>
                <li><span class="label label-success"><b>Sep 2020</b></span> <b style="color:red">Engineering Excellence Doctoral Fellowship</b> - I was awarded with the <a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/awards/engineering-excellence-masters-and-doctoral-fellowships-eemf#:~:text=Value%20description%3A-,EEDF,-%2D%20%24120%2C000%20paid%20across" target="_blank">EEDF</a> for my PhD work.</li>
                <li><span class="label label-success"><b>May 2019</b></span> <b>Internship</b> - Joined <a href="https://www.gm.com/stories/100-years-research-development" target="_blank">GM</a> at their Global R&D center as an AV Software Engineering Intern working on Deep Learning-based State Estimation.</li>
               <li><span class="label label-success"><b>May 2018</b></span> <b>Completed BASc</b> - I finished my Bachelor's program at University of Toronto and was admitted to a direct PhD program at University of Waterloo.</li>
                <li><span class="label label-success"><b>May 2017</b></span> <b>Internship</b> funded with <b style="color:red">NSERC Industrial Experience Award</b> - Joined <a href="https://clearpathrobotics.com/" target="_blank">Clearpath Robotics</a> as an R&D Appications Engineering Intern working on ROS projects.</li>
                <li><span class="label label-success"><b>May 2016</b></span> <b>Research Internship</b> funded with <b style="color:red">NSERC Undergraduate Research Award</b> - Worked with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/" target="_blank">Yu Sun</a> at the Robotics Institute of University of Toronto: Advanced Micro and Nanosystems Lab.</li>
            </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Publications</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/MLSys-24.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf" target="_blank">
                <papertitle>Fine-Tuning Language Models Using Formal Methods Feedback: A Use Case in Autonomous Systems</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang*</a>,
              <a href="https://oden.utexas.edu/people/directory/Tyler-Ingebrand/" target="_blank">Tyler Ingebrand*</a>,
              <a href="https://oden.utexas.edu/people/directory/William-Ward/" target="_blank">William Ward</a>,
              <a href="https://oden.utexas.edu/people/directory/Steven-Carr/" target="_blank">Steven Carr</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Conference on Machine Learning and Systems (MLSys), 2024</em>. 
              <br>
              <b style="color:red">One of only 37 accepted papers!</b>
              <br>
              [
              <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="posters/MLSys-24.pdf" target="_blank">Poster</a> &nbsp/&nbsp
              <a href="slides/MLSys-24.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=-N6ugXfHaFI&ab_channel=AsimMunawar" target="_blank">Video</a>
               ]
              <p>We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions that are formally verified. Controllers with high compliance of the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidence demonstrating an improvement in percentage of specifications satisfied from <strong>60%</strong> to <strong>90%</strong>.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MM3DGS.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2404.00923" target="_blank">
                <papertitle>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/lisong-codey-sun/" target="_blank">Lisong C. Sun*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://www.linkedin.com/in/jonathanliu88/" target="_blank">Jonathan C. Lu</a>,
              <a href="https://zhiwenfan.github.io/" target="_blank">Zhiwen Fan</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>,
              <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/humphreys" target="_blank">Todd E. Humphreys</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2024</em>.
              <br>
              <b style="color:red">Oral Pitch Finalist!</b>
              <br>
              [
              <a href="https://arxiv.org/pdf/2404.00923" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="posters/MM3DGS.pdf" target="_blank">Poster</a> &nbsp/&nbsp
              <a href="slides/MM3DGS.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=drf6UxehChE&ab_channel=NeelBhatt" target="_blank">Video</a> &nbsp/&nbsp
              <a href="https://vita-group.github.io/MM3DGS-SLAM/" target="_blank">Webpage</a>
               ]
              <p>Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. Experimental evaluation on several scenes shows a <strong>3x</strong> improvement in tracking and <strong>5%</strong> improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Comp4D_LLM.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2403.16993" target="_blank">
                <papertitle>Comp4D: LLM-Guided Compositional 4D Scene Generation</papertitle>
              </a>
              <br>
              <a href="https://ir1d.github.io/" target="_blank">Dejia Xu*</a>,
              <a href="https://scholar.google.com/citations?user=mrOHvI8AAAAJ&hl=en" target="_blank">Hanwen Liang*</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://alexhu.top/" target="_blank">Hezhen Hu</a>,
              <a href="https://scholar.google.com/citations?user=XcxDA14AAAAJ&hl=en/" target="_blank">Hanxue Liang</a>,
              <a href="https://www.ece.utoronto.ca/people/plataniotis-k-n/" target="_blank">Konstantinos N. Plataniotis</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>
              <br>
              <em>Under submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2403.16993" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=9q8SV1Xf_Xw&t=281s&ab_channel=DejiaXu" target="_blank">Video</a> &nbsp/&nbsp
              <a href="https://vita-group.github.io/Comp4D/" target="_blank">Webpage</a>
               ]
              <p>Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. Experimental evaluation on several scenes shows a <strong>3x</strong> improvement in tracking and <strong>5%</strong> improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Frontiers-24.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.readcube.com/articles/10.3389/frobt.2024.1212070" target="_blank">
                <papertitle>A Survey on 3D Object Detection in Real-time for Autonomous Driving</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=nUF0J2IAAAAJ&hl=en" target="_blank">Marcelo Contreras</a>,
              <a href="https://in.linkedin.com/in/aayush-jain-406315202" target="_blank">Aayush Jain</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://scholar.google.co.in/citations?user=Bol-rwwAAAAJ&hl=en" target="_blank">Arunava Banerjee</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Frontiers in Robotics and AI, 2024</em>. 
              <br>
              [
              <a href="https://www.readcube.com/articles/10.3389/frobt.2024.1212070" target="_blank">Paper</a>
               ]
              <p>We provide a comprehensive survey and quantitative comparisons with state-of-the-art 3D object detection methodologies aiming to tackle varying weather conditions, multi-modality, multi-camera perspective, and their respective metrics associated to different difficulty categories. We identify several research gaps and potential future directions in visual-based 3D object detection approaches for autonomous driving.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpc-pf_its.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">
                <papertitle>MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Transactions on Intelligent Transportation Systems (T-ITS), 2023</em>. 
              <br>
              <b style="color:red">Outperforms 2<sup>nd</sup> and 3<sup>rd</sup> place on Waymo Motion Dataset (2021)</b>
              <br>
              [
              <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=56sD-qsREBY&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=3&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p>We propose MPC-PF, a model that embeds surrounding object and road map information in the form of a potential field to model agent-agent and agent-space interactions. We show the efficacy of our multi-object trajectory prediction method both qualitatively and quantitatively achieving state-of-the-art results on the Waymo Open Motion Dataset and other common urban driving scenarios.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/integrated-inertial-lidar.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">
                <papertitle>Integrated Inertial-LiDAR-Based Map Matching Localization for Varying Environments</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=vCYqMTIAAAAJ&hl=en" target="_blank">Xin Xia</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Transactions on Intelligent Vehicles (T-IV), 2023</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=7HaQQtZYQh4&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p> Leveraging a global navigation satellite system, inertial navigation system, and 3D LiDAR point clouds, a novel light point cloud map generation method, which only keeps the necessary point clouds (i.e., buildings and roads regardless of vegetation varying with seasonal change), is proposed. Thorough experiments in winter and summer confirm the advantages of integrating the proposed light point cloud map generation with the dead reckoning model in terms of accuracy and reduced computational complexity.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SVO-23.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">
                <papertitle>A Stereo Visual Odometry Framework with Augmented Perception for Dynamic Urban Environments</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=nUF0J2IAAAAJ&hl=en" target="_blank">Marcelo Contreras</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2023</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="slides/SVO-23.pdf" target="_blank">Slides</a>
               ]
              <p> We propose a semantic-aware stereo visual odometry framework wherein feature extraction is performed over a static region-of-interest generated through object detection and instance segmentation on static street objects. Extensive real driving sequences in various dynamic urban environments with varying sequence lengths confirms excellent performance and computational efficiency attributed to using semantic-aware feature tracking.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/iros_poster.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046" target="_blank">
                <papertitle>MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=4DR7fqruLGo&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=1&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p>Predicting object motion behaviour is a challenging but crucial task for safe decision-making and planning for an autonomous vehicle. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model produces accurate short and long-term predictions.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/augmented_case.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">
                <papertitle>Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ali-salimzade" target="_blank">Ali Salimzadeh</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Automation Science and Engineering (CASE), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="slides/case_presentation.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://youtu.be/1bjuX6VuZjs" target="_blank">Video</a>
               ]
              <p>We develop a robust infrastructure-aided localization framework using only a single low-cost camera with a fisheye lens. To reduce the computational load, we use an ROI alongside estimated depth to re-project the robot pointcloud cluster with geometrical outlier detection. We use this position and depth information in an uncertainty-aware observer with adaptive covariance allocation and bounded estimation error to deal with position measurement noises at the limits of the field of view, and intermittent occlusion in dynamic environments. Moreover, we use a learning-based prediction model for input estimation based on a moving buffer of the robot position. Several experiments with occlusion and intermittent visual disruption/detection confirm effectiveness of the developed framework in re-initializing the estimation process after failure in the visual detection, and handling temporary data loss due to sensor faults or changes in lighting conditions.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infra_robotics_2.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.mdpi.com/2218-6581/11/4/82" target="_blank">
                <papertitle>Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://www.fzi.de/team/daniel-floegel/" target="_blank">Daniel Flögel</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Robotics Journal, 2022</em>. 
              <br>
              [
              <a href="https://www.mdpi.com/2218-6581/11/4/82" target="_blank">Paper</a>
               ]
              <p>A slip-aware localization framework is proposed for mobile robots experiencing wheel slip in dynamic environments. The framework fuses infrastructure-aided visual tracking data and proprioceptive sensory data from a skid-steer mobile robot to enhance accuracy and reduce variance of the estimated states. Covariance intersection is used to fuse the pose prediction and the visual thread, such that the updated estimate remains consistent. As confirmed by experiments on a skid-steer mobile robot, the designed localization framework addresses state estimation challenges for indoor/outdoor autonomous mobile robots which experience high-slip, uneven torque distribution at each wheel (by the motion planner), or occlusion when observed by an infrastructure-mounted camera.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpr_smcs.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.06901" target="_blank">
                <papertitle>Soft Constrained Autonomous Vehicle Navigation using Gaussian Processes and Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=yv016ZMAAAAJ&hl=en" target="_blank">Bruno H. Groenner Barbosa</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Submitted to Transactions on Systems, Man, and Cybernetics: Systems (SMCS)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2101.06901" target="_blank">Paper</a>
               ]
              <p>We present a generic feature-based navigation framework for autonomous vehicles using a soft constrained Particle Filter. After obtaining features of mapped landmarks in instance-based segmented images acquired from a monocular camera, vehicle-to-landmark distances are predicted using Gaussian Process Regression (GPR) models in a mixture of experts approach. Experimental results confirm that the use of image segmentation features improves the vehicle-to-landmark distance prediction notably, and that the proposed soft constrained approach reliably localizes the vehicle even with reduced number of landmarks and noisy observations.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhe_itsc.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">
                <papertitle>Real-time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ehsan-mohammadbagher" target="_blank">Ehsan Mohammadbagher*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/fidan" target="_blank">Baris Fidan</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2020</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=2ORorrOtIfE&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=4&ab_channel=NeelBhatt" target="_blank">Video</a>  
               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>
               
        </tbody></table>

        <h2>Featured Projects</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">
                <papertitle>WATonoBus: A Brief Demo of Canada's First Autonomous 5G Shuttle</papertitle>
              </a>
              <br>
              [
              <a href="https://arxiv.org/pdf/2312.00938" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://uwaterloo.ca/watonobus/" target="_blank">Webpage</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=G0T5sKgqpLM&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=2&ab_channel=WaterlooEngineering" target="_blank">Video</a>               ]
              <p>Our team started the WATonoBus autonomous shuttle project in 2018 and has since developed 2 such fully equipped shuttles and is near completion of the third shuttle. The WATonoBus is a platform that contains fully in-house equipped hardware and software stack and has been approved as part of the Ministry of Transportation Ontario’s Autonomous Vehicle Pilot Program currently providing daily free and fully autonomous service to passengers at the University of Waterloo. The University of Waterloo's Ring Road is a 2.7 km curvy road with many intersections and pedestrian crossings that represent an urban driving environment with several pedestrians, cyclists, and vehicles. The WATonoBus project is different from prior project in that it is aimed to operate in all weather conditions including adverse rain, fog, and snow.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/node_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home" target="_blank">
                <papertitle>NODE: A Brief Demo of UAlberta's Autonomous Vehicle</papertitle>
              </a>
              <br>
              [
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home?authuser=0" target="_blank">Webpage</a> &nbsp/&nbsp
              <a href="https://youtu.be/bnfe1BZZiaw?t=52" target="_blank">Video</a>               ]
              <p>Since 2021, I have been leading efforts at the NODE lab to develop hardware and software stack on the NODE lab's autonomous Ford Escape vehicle. This platform has been central in working on several research projects covering RL, visual odometry, SLAM, object detection. I also supervise several PhD and Masters students at the lab.
              </p>
            </td>
          </tr>

        </tbody></table>

        <h2>Outreach</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TACC_outreach_combined.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://public.cyber.mil/gencyber/" target="_blank">
                <papertitle>GenCyber Back@TACC Coding and Cybersecurity Camp / TACC and Center for Autonomy</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/news/center-autonomy-volunteers-gencyber-tacc-camps" target="_blank">Webpage</a>               ]
              <p>In December of 2023, the Texas Advanced Computing Center (TACC) hosted two engaging cybersecurity events, "GenCyber Back@TACC" and "Level UP GenCyber Back@TACC," both aimed at educating high school students in various aspects of coding and cybersecurity. I contributed by motivating students envision a future in both cybersecurity and the broader selection of STEM fields. I also helped by facilitating the learning process, answering questions, and engaging high school students in discussions about their personal experiences from their academic journeys in STEM, thereby encouraging students to consider future studies and careers.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/High_school_FRC_outreach.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://autonomy.oden.utexas.edu/learning-together-del-valle-students-experience-center-autonomy" target="_blank">
                <papertitle>Del Valle High School FRC Team Tours / TACC and Center for Autonomy</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/learning-together-del-valle-students-experience-center-autonomy" target="_blank">Webpage</a>               ]
              <p>This event, hosted by Center for Autonomy, invited students from the Del Valle Independent School District (Travis County, TX), out of which 84.4% are economically disadvantaged. I presented to the students a high-level overview of what academic research looks like through my research on multi-modal 3D Guassian splatting for simultaneous localization and mapping and its implementation on the Clearpath jackal robots. I used TACC’s high resolution screens to deliver my presentation in an interactive manner. I shared my undergrad, internship, PhD, and postdoc experiences with the students and gave them a personal view of how each one led to the next career step so that they could understand potential career trajectories. I motivated the students to ask questions and answered their questions ranging from how I got into robotics through First Robotics Challenge (FRC) – their school participates in the same competition, what I do not like about industry, and failures through my career journey.
              </p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template adopted from <a href="https://jonbarron.info/" target="_blank">here</a>.
</body>

</html>
