<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Neel P. Bhatt</title>

  <meta name="description" content="Neel P. Bhatt's personal website" />
  <meta name="author" content="Neel P. Bhatt">
  <meta name="keywords" content="Neel P. Bhatt, Neel Bhatt, Autonomous Driving, Robotics, Computer Vision" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/uw_logo.png">
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CHJT8YCW79"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CHJT8YCW79');
</script>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b>Neel</b> P. Bhatt</name>
              </p>
              <p>I am currently a Postdoctoral Fellow at the <a href="https://autonomy.oden.utexas.edu/">Center for Autonomy</a> working with Professors <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang">Atlas Wang</a> and <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/">Ufuk Topcu</a>. I am part of the <a href="https://autonomy.oden.utexas.edu/Groups/autonomous-systems-group">Autonomous Systems Group</a> and the <a href="https://vita-group.github.io/">VITA Research Group</a> at the University of Texas at Austin. My research is centered at the intersection of generative AI, assured active perception, prediction, and trustworthy sequential decision making for autonomous systems.</p>
              <p>I received my PhD in Mechatronics Engineering from University of Waterloo in 2023. During my PhD, I led efforts on the <a href="https://uwaterloo.ca/watonobus/">WATonoBus project</a> at <a href="https://uwaterloo.ca/mechatronic-vehicle-systems-lab/">MVS Lab</a> working on software and algorithmic development of perception and prediction modules required for <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless">Canada’s first autonomous shuttle bus</a> approved via the ministry’s autonomous vehicle pilot. During this time, I also interned at <a href="https://www.gm.com/company/facilities/global-technical-center">GM R&D</a> where I worked on deep learning-based state estimation.</p>
              <p>I have also concurrently been a visiting research scholar since 2021 at the <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home">NODE Lab</a> at University of Alberta working on NODE lab's autonomous vehicle.</p>
              <p> Prior to this, I received my Bachelor's degree in Mechanical Engineering (yes, I was directly admitted to a PhD from Bachelor's) in 2018 with focus on Mechatronics and Robotics from University of Toronto. During this time, I conducted research with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/">Yu Sun</a> on micro and nano robotics and interned at <a href="https://clearpathrobotics.com/">Clearpath Robotics</a> and was awarded <a href="https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp">NSERC Research Awards</a> for both.</p>
              <p style="text-align:center">
                    [
                <a href="mailto:npbhatt@utexas.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=S8ofWDUAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/neelbhattportfolio/">LinkedIn</a> &nbsp/&nbsp
                <a href="cv/CV_Neel_Bhatt_November_2023.pdf">CV</a>
]
              </p>
            </td>
            <td style="padding:1.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_v2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_v2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                <li><b>November 2023</b>: <b>Paper Preprint</b> - Our paper on <a href="https://arxiv.org/pdf/2310.18239.pdf">Fine-tuning Language Models Using Formal Methods Feedback</a> is up!</li>
                <li><b>October 2023</b>: <b>Talk @ UC Berkeley</b> - I gave a talk on my work at the <a href="https://www.darpa.mil/program/assured-neuro-symbolic-learning-and-reasoning">DARPA ANSR</a> PI meeting at UC Berkeley.</li>
                <li><b>September 2023</b>: <b>Organizing Committee</b> - I am part of the organizing committee for <a href="https://ieee-itsc.org/2024/">ITSC 2024</a>.</li>
                <li><b>September 2023</b>: <b style="color:red">Started as a Postdoc</b> - I started as a Postdoctoral Fellow at <a href="https://oden.utexas.edu/">The University of Texas at Austin</a>!</li>
                <li><b>August 2023</b>: <b>Paper Accepted @ ITSC 2023</b> - Our work DynaStreet-SVO: A Stereo Visual Odometry Framework with Augmented Perception for Dynamic Urban Environments was accepted to ITSC 2023.</li>
                <li><b>July 2023</b>: <b>Paper Published @ T-IV 2023</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/10194984">Integrated Inertial-LiDAR-Based Map Matching Localization for Varying Environments</a> was published in T-IV 2023.</li>
                <li><b>June 2023</b>: <b>Featured Video</b> - I was featured on the <a href="https://www.ualberta.ca/research/our-research/artificial-intelligence.html">homepage</a> of Artificial Intelligence Research and Innovation at University of Alberta via a <a href="https://youtu.be/bnfe1BZZiaw?t=52">promo video</a>. I also discussed about how I use AI in my research via this U of A <a href="https://www.linkedin.com/posts/university-of-alberta_ai-innovator-neel-bhatt-activity-7084575724824629248-NH1-?utm_source=share&utm_medium=member_desktop">video</a> and <a href="https://www.ualberta.ca/the-quad/2023/06/innovator-spotlight-neel-bhatt.html">article</a>.</li>
                <li><b>June 2023</b>: <b>Workshop</b> - Co-organizing a workshop <a href="https://sites.google.com/view/uofatutorialiv23/home">Reliable State Estimation and Distributed Controls in Intelligent Vehicular Networks</a> at IV 2023.</li>
                <li><b>May 2023</b>: <b style="color:red">Defended PhD Thesis</b> - My thesis work is out on <a href="https://uwspace.uwaterloo.ca/handle/10012/19210">UW Space</a>.</li> 
                <li><b>May 2023</b>: <b>Paper Published @ T-ITS 2023</b> - Our work <a href="https://ieeexplore.ieee.org/document/10046400">MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</a> was accepted to T-ITS 2023.</li> 
                <li><b>April 2023</b>: <b>Guest Lecture</b> - Gave a guest lecture on WATonoBus ‑ Algorithms and Software Structure for an All Weather Shuttle for ECE495 at University of Waterloo.</li> 
                <li><b>October 2022</b>: <b>Paper Published @ IROS 2022</b> - I will be presenting our work <a href="https://sites.google.com/view/neverendingrl/home?authuser=0">MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</a> at IROS 2022 in Kyoto.</li>
                <li><b>August 2022</b>: <b>Paper Published @ CASE 2022</b> - Our work on <a href="https://www.mdpi.com/1782890">Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</a> is accepted to CASE 2022.</li>
                <li><b>August 2022</b>: <b>Paper Published @ Robotics 2022</b> - Our work on <a href="https://ieeexplore.ieee.org/abstract/document/9926595">Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</a> is accepted to Robotics 2022.</li>
                <li><b>June 2021</b>: <b>WATonoBus Project</b> - The <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless">autonomous shuttle project</a> I am leading at MVS Lab was given approval as part of the ministry's autonomous vehicle pilot.</li>
                <li><b>September 2020</b>: <b>Paper Published @ ITSC 2020</b> - I will be presenting our work <a href="https://ieeexplore.ieee.org/abstract/document/9294306">Real‑time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</a> at ITSC 2020 in Greece.</li>
                <li><b>September 2020</b>: <b style="color:red">Engineering Excellence Doctoral Fellowship</b> - I was awarded with the <a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/awards/engineering-excellence-masters-and-doctoral-fellowships-eemf#:~:text=Value%20description%3A-,EEDF,-%2D%20%24120%2C000%20paid%20across">EEDF</a> for my PhD work.</li>
                <li><b>May 2019</b>: <b>Internship</b> - Joined <a href="https://www.gm.com/stories/100-years-research-development">GM</a> at their Global R&D center as an AV Software Engineering Intern working on Deep Learning-based State Estimation.</li>
               <li><b>May 2018</b>: <b>Completed BASc</b> - I finished my Bachelor's program at University of Toronto and was admitted to a direct PhD program at University of Waterloo.</li>
                <li><b>May 2017</b>: <b>Internship</b> funded with <b style="color:red">NSERC Industrial Experience Award</b> - Joined <a href="https://clearpathrobotics.com/">Clearpath Robotics</a> as an R&D Appications Engineering Intern working on ROS projects.</li>
                <li><b>May 2016</b>: <b>Research Internship</b> funded with <b style="color:red">NSERC Undergraduate Research Award</b> - Worked with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/">Yu Sun</a> at the Robotics Institute of University of Toronto: Advanced Micro and Nanosystems Lab.</li>
            </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Publications</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpc-pf_its.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10046400">
                <papertitle>MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>
              <br>
              <em>Transactions on Intelligent Transportation Systems (T-ITS), 2023</em>. 
              <br>
              <b style="color:red">Outperforms 2<sup>nd</sup> and 3<sup>rd</sup> place on Waymo Motion Dataset (2021)</b>
              <br>
              [
              <a href="https://ieeexplore.ieee.org/document/10046400">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=56sD-qsREBY&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=3&ab_channel=NeelBhatt">Video</a>
               ]
              <p>We propose MPC-PF, a model that embeds surrounding object and road map information in the form of a potential field to model agent-agent and agent-space interactions. We show the efficacy of our multi-object trajectory prediction method both qualitatively and quantitatively achieving state-of-the-art results on the Waymo Open Motion Dataset and also on other common urban driving scenarios where social norms must be followed.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros_poster.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">
                <papertitle>MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=4DR7fqruLGo&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=1&ab_channel=NeelBhatt">Video</a>
               ]
              <p>Predicting object motion behaviour is a challenging but crucial task for safe decision making and path planning for an autonomous vehicle. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model is capable of producing accurate predictions for short and long term timesteps and demonstrate significance of model architecture via an ablation study.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/augmented_case.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">
                <papertitle>Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ali-salimzade">Ali Salimzadeh</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Automation Science and Engineering (CASE), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">Paper</a> &nbsp/&nbsp
              <a href="https://youtu.be/1bjuX6VuZjs">Video</a> &nbsp/&nbsp
              <a href="slides/case_presentation.pdf">Slides</a>
               ]
              <p>We develop a robust infrastructure-aided localization framework using only a single low-cost camera with a fisheye lens. To reduce the computational load, we use an ROI alongside estimated depth to re-project the robot pointcloud cluster with geometrical outlier detection. We use this position and depth information in an uncertainty-aware observer with adaptive covariance allocation and bounded estimation error to deal with position measurement noises at the limits of the field of view, and intermittent occlusion in dynamic environments. Moreover, the aforementioned motion model, uses a learning-based prediction model for input estimation based on a moving buffer of the robot position. Several experiments with occlusion and intermittent visual disruption/detection confirm effectiveness of the developed framework in re-initializing the estimation process after failure in the visual detection, and handling temporary data loss due to sensor faults or changes in lighting conditions.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infra_robotics_2.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/2218-6581/11/4/82">
                <papertitle>Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://www.fzi.de/team/daniel-floegel/">Daniel Flögel</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>
              <br>
              <em>Robotics Journal, 2022</em>. 
              <br>
              [
              <a href="https://www.mdpi.com/2218-6581/11/4/82">Paper</a>
               ]
              <p>A slip-aware localization framework is proposed for mobile robots experiencing wheel slip in dynamic environments. The framework fuses infrastructure-aided visual tracking data and proprioceptive sensory data from a skid-steer mobile robot to enhance accuracy and reduce variance of the estimated states. Covariance intersection is used to fuse the pose prediction and the visual thread, such that the updated estimate remains consistent. As confirmed by experiments on a skid-steer mobile robot, the designed localization framework addresses state estimation challenges for indoor/outdoor autonomous mobile robots which experience high-slip, uneven torque distribution at each wheel (by the motion planner), or occlusion when observed by an infrastructure-mounted camera.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpr_smcs.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.06901">
                <papertitle>Soft Constrained Autonomous Vehicle Navigation using Gaussian Processes and Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=yv016ZMAAAAJ&hl=en">Bruno H. Groenner Barbosa</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>
              <br>
              <em>Submitted to Transactions on Systems, Man, and Cybernetics: Systems (SMCS)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2101.06901">Paper</a>
               ]
              <p>We present a generic feature-based navigation framework for autonomous vehicles using a soft constrained Particle Filter. After obtaining features of mapped landmarks in instance-based segmented images acquired from a monocular camera, vehicle-to-landmark distances are predicted using Gaussian Process Regression (GPR) models in a mixture of experts approach. Experimental results confirm that the use of image segmentation features improves the vehicle-to-landmark distance prediction notably, and that the proposed soft constrained approach reliably localizes the vehicle even with reduced number of landmarks and noisy observations.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhe_itsc.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>Real-time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ehsan-mohammadbagher">Ehsan Mohammadbagher*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/fidan">Baris Fidan</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2020</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=2ORorrOtIfE&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=4&ab_channel=NeelBhatt">Video</a>  
               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>
               
        </tbody></table>

        <h2>Featured Projects</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>WATonoBus: A Brief Demo of Canada's First Autonomous 5G Shuttle</papertitle>
              </a>
              <br>
              [
              <a href="https://uwaterloo.ca/watonobus/">Webpage</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=G0T5sKgqpLM&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=2&ab_channel=WaterlooEngineering">Video</a>               ]
              <p>Our team started the WATonoBus autonomous shuttle project in 2018 and has since developed 2 such fully equipped shuttles and is near completion of the third shuttle. The WATonoBus is a platform that contains fully in-house equipped hardware and software stack and has been approved as part of the Ministry of Transportation Ontario’s Autonomous Vehicle Pilot Program currently providing daily free and fully autonomous service to passengers at the University of Waterloo. The University of Waterloo's Ring Road is a 2.7 km curvy road with many intersections and pedestrian crossings that represent an urban driving environment with several pedestrians, cyclists, and vehicles. The WATonoBus project is different from prior project in that it is aimed to operate in all weather conditions including adverse rain, fog, and snow.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/node_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home">
                <papertitle>NODE: A Brief Demo of UAlberta's Autonomous Vehicle</papertitle>
              </a>
              <br>
              [
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home?authuser=0">Webpage</a> &nbsp/&nbsp
              <a href="https://youtu.be/bnfe1BZZiaw?t=52">Video</a>               ]
              <p>Since 2021, I have been leading efforts at the NODE lab to develop hardware and software stack on the NODE lab's autonomous Ford Escape vehicle. This platform has been central in working on several research projects covering RL, visual odometry, SLAM, object detection. I also supervise several PhD and Masters students at the lab.
              </p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>
