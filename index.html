<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Neel P. Bhatt</title>

  <meta name="description" content="Neel P. Bhatt's personal website" />
  <meta name="author" content="Neel P. Bhatt">
  <meta name="keywords" content="Neel P. Bhatt, Neel Bhatt, Autonomous Driving, Robotics, Computer Vision, AI, ML, Austin, Canada" />
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Latest compiled and minified CSS -->
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css"> -->

  <!-- jQuery library -->
  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script> -->

  <!-- Latest compiled JavaScript -->
  <!-- <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/profile_pic_v2.png">
  <script src="https://cdn.jsdelivr.net/npm/particles.js@2.0.0/particles.min.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CHJT8YCW79"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CHJT8YCW79');
</script>

<body>
  <div id="network-bg"></div>
  
  <!-- Tab Navigation -->
  <div class="tab-navigation" style="position: sticky; top: 0; z-index: 100; background-color: #fff; padding: 8px 0; margin-bottom: 15px;">
    <div style="max-width: 900px; margin: 0 auto; padding: 0 20px;">
      <button class="tab-btn active" onclick="scrollToSection('section-news', this)">News</button>
      <button class="tab-btn" onclick="scrollToSection('section-projects', this)">Projects</button>
      <button class="tab-btn" onclick="scrollToSection('section-publications', this)">Publications</button>
      <button class="tab-btn" onclick="scrollToSection('section-outreach', this)">Outreach</button>
      <button class="tab-btn" onclick="scrollToSection('section-media', this)">Media</button>
    </div>
  </div>

  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name><b>Neel</b> P. Bhatt</name>
              </p>
              <p>I am currently a <a href="https://autonomy.oden.utexas.edu/profiles/neel-bhatt" target="_blank">Research Scientist</a> at the <a href="https://autonomy.oden.utexas.edu/" target="_blank">Center for Autonomy</a> working with Professors <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a> and <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>. I am part of the <a href="https://autonomy.oden.utexas.edu/Groups/autonomous-systems-group" target="_blank">Autonomous Systems Group</a> and the <a href="https://vita-group.github.io/" target="_blank">VITA Research Group</a> at the University of Texas at Austin. I am developing neuro-symbolic architectures with research centered at the intersection of generative AI, assured active perception, prediction, and trustworthy sequential decision making for autonomous systems. At UT, I lead two DARPA projects on verification-centric neurosymbolic AI for embodied autonomy.</p>
              <p>I received my PhD in Mechatronics Engineering from University of Waterloo in 2023. During my PhD, I led efforts on the <a href="https://uwaterloo.ca/watonobus/" target="_blank">WATonoBus project</a> at <a href="https://uwaterloo.ca/mechatronic-vehicle-systems-lab/" target="_blank">MVS Lab</a> working on software and algorithmic development of perception and prediction modules required for <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">Canada’s first autonomous shuttle bus</a> approved via the ministry’s autonomous vehicle pilot. During this time, I also interned at <a href="https://www.gm.com/company/facilities/global-technical-center" target="_blank">GM R&D</a> where I worked on deep learning-based state estimation.</p>
              <p>I have also concurrently been a visiting research scholar since 2021 at the <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home" target="_blank">NODE Lab</a> at University of Alberta working on NODE lab's autonomous vehicle.</p>
              <p> Prior to this, I received my Bachelor's degree in Mechanical Engineering (I was directly admitted to a PhD from Bachelor's) in 2018 with focus on Mechatronics and Robotics from University of Toronto. During this time, I conducted research with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/" target="_blank">Yu Sun</a> on micro and nano robotics and interned at <a href="https://clearpathrobotics.com/" target="_blank">Clearpath Robotics</a> and was awarded <a href="https://www.nserc-crsng.gc.ca/students-etudiants/ug-pc/usra-brpc_eng.asp" target="_blank">NSERC Research Awards</a> for both.</p>
              <p style="text-align:center">
                    [
                <a href="mailto:npbhatt@utexas.edu" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/outlook_icon.png"> Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=S8ofWDUAAAAJ&view_op=list_works&sortby=pubdate" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/scholar_icon.png"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/neelbhattportfolio/" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/linkedin_icon.png"> LinkedIn</a> &nbsp/&nbsp
                <a href="cv/CV_Neel_Bhatt_Dec_2025.pdf" target="_blank"><img style="height:15px; margin-bottom:-1px" src="images/cv_logo.png"> CV</a>
]
              </p>
            </td>
            <td style="padding:1.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_v2.png" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_v2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>

        <!-- News Section -->
        <div id="section-news" class="content-section">
        <p style="text-align: center; font-style: italic; margin-top: -20px; margin-bottom: 20px; color: #555;">I am open to discussions regarding faculty opportunities starting in 2026.</p>
        <h2>News</h2>

            <div class="news-container">
            <ul>
                <li><span class="label label-paper"><b>Nov 2025</b></span> <b>Book Chapter on Neurosymbolic AI</b> - Our <a href="https://www.wiley.com/en-us/Neuro-symbolic+AI%3A+Foundations+and+Applications-p-9781394302376" target="_blank">Neurosymbolic AI: Foundations and Applications</a> book chapter will be published via Wiley in December!
                <li><span class="label label-talk"><b>Nov 2025</b></span> <b>Invited Talk @ Neurosymbolic AI Course at UCB</b> - I was invited as a guest lecturer at a Neurosymbolic AI course at UC Boulder!
                <li><span class="label label-talk"><b>Oct 2025</b></span> <b>Invited Talk @ Neurosymbolic AI Course at CMU</b> - I was invited as a guest lecturer at a <a href="https://www.cs.cmu.edu/~pradeepr/747/" target="_blank">Neurosymbolic AI course</a> at a CMU!
                <li><span class="label label-talk"><b>Oct 2025</b></span> <b>Talk @ Lockheed Martin Advanced Technology Lab</b> - I gave a talk at <a href="https://www.cs.cmu.edu/~pradeepr/747/" target="_blank">Lockheed Martin ATL - Trusted Intelligent Systems</a> during their annual project PI meeting.
                <li><span class="label label-talk"><b>Oct 2025</b></span> <b>Invited Talk @ FM4Control Workshop</b> - I was invited as a workshop field expert/speaker at the <a href="https://sites.google.com/andrew.cmu.edu/fm4control-workshop" target="_blank">FM4Control Workshop</a> at the CMU!
                <li><span class="label label-milestone"><b>July 2025</b></span> <b>Research Scientist @ Oden Institute</b> - I was appointed as a Research Scientist at the <a href="https://oden.utexas.edu/" target="_blank">Oden Institute</a> working at the Center for Autonomy at UT Austin!
                <li><span class="label label-talk"><b>June 2025</b></span> <b>Talk @ Vanderbilt University for DARPA ANSR</b> - I gave a talk on "NeuroSymbolic LoRA" at the DARPA ANSR PI meeting #4 at Vanderbilt University.
                <li><span class="label label-talk"><b>June 2025</b></span> <b>Research Pitch @ Washington DC for AI Expo</b> - I presented a research pitch at the <a href="https://expo.scsp.ai/" target="_blank">AI Expo</a> in Washington.
                <li><span class="label label-talk"><b>May 2025</b></span> <b>Featured Talk @ GrapEx MIT LL</b> - I gave a talk on "In-context automated refinement of LLMs and VLMs" transfer at <a href="https://graphex.mit.edu/" target="_blank">GraphEx 2025</a> at the MIT's Endicott House.
                <li><span class="label label-talk"><b>Mar 2025</b></span> <b>Talk @ UC Berkeley for DARPA TIAMAT</b> - I gave a talk on "In-context automated refinement of LLMs and VLMs" transfer at the DARPA TIAMAT PI meeting #2 at UC Berkeley.
                <li><span class="label label-paper"><b>Feb 2025</b></span> <b>Paper Accepted @ PNAS Nexus</b> - Our position paper on <a href="https://drive.google.com/file/d/1WxHhQDAiVpyvRecTxjLtLTWy5VewWtOF/view?usp=drive_link" target="_blank">Neuro-symbolic AI as an antithesis to scaling laws</a> was accepted at PNAS Nexus. (Stay tuned for the final version of the paper.)</li>
                <li><span class="label label-paper"><b>Feb 2025</b></span> <b>Paper Accepted @ MLSys 2025</b> - Our work on <a href="https://arxiv.org/abs/2411.01639" target="_blank">Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework</a> was accepted at MLSys 2025. <b style="color:red">One of only 61 accepted papers!</b></a></li>
                <li><span class="label label-talk"><b>Nov 2024</b></span> <b>Talk @ SRI International for DARPA ANSR</b> - I gave a talk on "Neurosymbolic Foundation Model Training at Scale" at the DARPA ANSR PI meeting #3 at SRI International.
                <li><span class="label label-paper"><b>Oct 2024</b></span> <b>Paper Accepted @ NeurIPS Workshop 2024</b> - Our work on <a href="https://arxiv.org/pdf/2409.19924" target="_blank">On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability</a> was accepted at the Language Gamification workshop @ NeurIPS 2024.</li>
                <li><span class="label label-talk"><b>Sep 2024</b></span> <b>Talk @ DARPA TIAMAT/Indianapolis Autonomous Indy</b> - I gave a talk on compositional Sim2Real transfer at the <a href="https://www.darpa.mil/program/transfer-from-imprecise-and-abstract-models-to-autonomous-technologies" target="_blank">DARPA TIAMAT</a> PI kick-off meeting #1 at Indianapolis motor speedway.</li>
                <li><span class="label label-paper"><b>Jul 2024</b></span> <b>Paper Accepted @ T-IV 2024</b> - Our work on <a href="https://ieeexplore.ieee.org/abstract/document/10561598" target="_blank">DynaNav-SVO: Dynamic Stereo Visual Odometry With Semantic-Aware Perception for Autonomous Navigation</a> was accepted at T-IV 2024.</li>
                <li><span class="label label-paper"><b>Jun 2024</b></span> <b>Paper Accepted @ IROS 2024</b> - Our work on <a href="https://arxiv.org/pdf/2404.00923" target="_blank">MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements</a> was accepted at IROS 2024. <b style="color:red">Oral Pitch Finalist!</b></li>
                <li><span class="label label-talk"><b>May 2024</b></span> <b>Talk @ DESTION Workshop and HP AI Labs</b> - I will be giving a talk at <a href="https://cps-vo.org/group/DESTION2024/program" target="_blank">DESTION workshop</a> and HP AI Labs on our fine-tuning work.</li>
                <li><span class="label label-talk"><b>May 2024</b></span> <b>Serving as Associate Editor @ ITSC 2024</b> - I will be serving as an associate editor for <a href="https://ieee-itsc.org/2024/" target="_blank">ITSC 2024</a> [<a href="https://www.youtube.com/watch?v=SjlhfRUpOvA&ab_channel=NeelBhatt/" target="_blank">Video</a>].</li>
                <li><span class="label label-talk"><b>Apr 2024</b></span> <b>Talk @ CMU</b> - I gave a talk on assured compositional RL for area and route search missions at the <a href="https://www.darpa.mil/program/assured-neuro-symbolic-learning-and-reasoning" target="_blank">DARPA ANSR</a> PI meeting #2 at CMU.</li>
                <li><span class="label label-paper"><b>Mar 2024</b></span> <b>Paper Preprint</b> - Our paper on <a href="https://arxiv.org/pdf/2403.16993" target="_blank">Comp4D: LLM-Guided Compositional 4D Scene Generation</a> is up!</li>
                <li><span class="label label-paper"><b>Feb 2024</b></span> <b>Paper Accepted @ MLSys 2024</b> - Our work on <a href="https://arxiv.org/pdf/2310.18239" target="_blank">Fine-tuning Language Models Using Formal Methods Feedback: A Use Case in Autonomous Systems</a> was accepted at: <a href="https://mlsys.org/virtual/2024/session/2779#:~:text=Fine%2DTuning%20Language%20Models%20Using%20Formal%20Methods%20Feedback%3A%20A%20Use%20Case%20in%20Autonomous%20Systems" target="_blank">MLSys 2024. <b style="color:red">One of only 37 accepted papers!</b></a></li>
                <li><span class="label label-talk"><b>Jan 2024</b></span> <b>Talk @ Oxford</b> - I gave a talk at Oxford on our fine-tuning work.</li>
                <li><span class="label label-paper"><b>Dec 2023</b></span> <b>Paper Accepted @ AAAI Workshop 2024</b> - Our work on <a href="https://arxiv.org/pdf/2310.18239" target="_blank">Fine-tuning Language Models Using Formal Methods Feedback</a> was accepted at the <a href="https://nuclear-workshop.github.io/" target="_blank">Neuro-Symbolic Learning and Reasoning in the Era of Large Language Models</a> workshop at AAAI 2024</li>
                <li><span class="label label-talk"><b>Oct 2023</b></span> <b>Talk @ UC Berkeley</b> - I gave a talk on my work at the <a href="https://www.darpa.mil/program/assured-neuro-symbolic-learning-and-reasoning" target="_blank">DARPA ANSR</a> PI meeting #1 at UC Berkeley.</li>
                <li><span class="label label-talk"><b>Sep 2023</b></span> <b>Organizing Committee</b> - I am part of the organizing committee for <a href="https://ieee-itsc.org/2024/" target="_blank">ITSC 2024</a>.</li>
                <li><span class="label label-milestone"><b>Sep 2023</b></span> <b style="color:red">Started as a Postdoc</b> - I started as a Postdoctoral Fellow at <a href="https://oden.utexas.edu/" target="_blank">The University of Texas at Austin</a>!</li>
                <li><span class="label label-paper"><b>Aug 2023</b></span> <b>Paper Accepted @ ITSC 2023</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">A Stereo Visual Odometry Framework with Augmented Perception for Dynamic Urban Environments</a> was accepted to <a href="https://2023.ieee-itsc.org/" target="_blank">ITSC 2023</a>.</li>
                <li><span class="label label-paper"><b>Jul 2023</b></span> <b>Paper Published @ T-IV 2023</b> - Our work <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">Integrated Inertial-LiDAR-Based Map Matching Localization for Varying Environments</a> was published in T-IV 2023.</li>
                <li><span class="label label-talk"><b>Jun 2023</b></span> <b>Featured Video</b> - I was featured on the <a href="https://www.ualberta.ca/research/our-research/artificial-intelligence.html" target="_blank">homepage</a> of Artificial Intelligence Research and Innovation at University of Alberta via a <a href="https://youtu.be/bnfe1BZZiaw?t=52" target="_blank">promo video</a>. I also discussed about how I use AI in my research via this U of A <a href="https://www.linkedin.com/posts/university-of-alberta_ai-innovator-neel-bhatt-activity-7084575724824629248-NH1-?utm_source=share&utm_medium=member_desktop" target="_blank">video</a> and <a href="https://www.ualberta.ca/the-quad/2023/06/innovator-spotlight-neel-bhatt.html" target="_blank">article</a>.</li>
                <li><span class="label label-talk"><b>Jun 2023</b></span> <b>Workshop @ IV 2023</b> - Co-organizing a workshop on <a href="https://sites.google.com/view/uofatutorialiv23/home" target="_blank">Reliable State Estimation and Distributed Controls in Intelligent Vehicular Networks</a> at <a href="https://2023.ieee-iv.org/" target="_blank">IV 2023</a>.</li>
                <li><span class="label label-milestone"><b>May 2023</b></span> <b style="color:red">Defended PhD Thesis</b> - My thesis work is out on <a href="https://uwspace.uwaterloo.ca/handle/10012/19210" target="_blank">UW Space</a>.</li> 
                <li><span class="label label-paper"><b>May 2023</b></span> <b>Paper Published @ T-ITS 2023</b> - Our work <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</a> was accepted to T-ITS 2023.</li> 
                <li><span class="label label-talk"><b>Apr 2023</b></span> <b>Guest Lecture</b> - Gave a guest lecture on WATonoBus ‑ Algorithms and Software Structure for an All Weather Shuttle for ECE495 at University of Waterloo.</li> 
                <li><span class="label label-paper"><b>Oct 2022</b></span> <b>Paper Published @ IROS 2022</b> - I will be presenting our work <a href="https://sites.google.com/view/neverendingrl/home?authuser=0" target="_blank">MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</a> at IROS 2022 in Kyoto. <b style="color:red">SOTA on Waymo Motion Prediction Dataset!</b></li>
                <li><span class="label label-paper"><b>Aug 2022</b></span> <b>Paper Published @ CASE 2022</b> - Our work on <a href="https://www.mdpi.com/1782890" target="_blank">Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</a> was accepted to CASE 2022.</li>
                <li><span class="label label-paper"><b>Aug 2022</b></span> <b>Paper Published @ Robotics 2022</b> - Our work on <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</a> was accepted to Robotics 2022.</li>
                <li><span class="label label-talk"><b>Jun 2021</b></span> <b>WATonoBus Project</b> - The <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">autonomous shuttle project</a> I am leading at MVS Lab was given approval as part of the ministry's autonomous vehicle pilot.</li>
                <li><span class="label label-paper"><b>Sep 2020</b></span> <b>Paper Published @ ITSC 2020</b> - I will be presenting our work <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">Real‑time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</a> at ITSC 2020 in Greece.</li>
                <li><span class="label label-milestone"><b>Sep 2020</b></span> <b style="color:red">Engineering Excellence Doctoral Fellowship</b> - I was awarded with the <a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/awards/engineering-excellence-masters-and-doctoral-fellowships-eemf#:~:text=Value%20description%3A-,EEDF,-%2D%20%24120%2C000%20paid%20across" target="_blank">EEDF</a> for my PhD work.</li>
                <li><span class="label label-milestone"><b>May 2019</b></span> <b>Internship</b> - Joined <a href="https://www.gm.com/stories/100-years-research-development" target="_blank">GM</a> at their Global R&D center as an AV Software Engineering Intern working on Deep Learning-based State Estimation.</li>
               <li><span class="label label-milestone"><b>May 2018</b></span> <b>Completed BASc</b> - I finished my Bachelor's program at University of Toronto and was admitted to a direct PhD program at University of Waterloo.</li>
                <li><span class="label label-milestone"><b>May 2017</b></span> <b>Internship</b> funded with <b style="color:red">NSERC Industrial Experience Award</b> - Joined <a href="https://clearpathrobotics.com/" target="_blank">Clearpath Robotics</a> as an R&D Appications Engineering Intern working on ROS projects.</li>
                <li><span class="label label-milestone"><b>May 2016</b></span> <b>Research Internship</b> funded with <b style="color:red">NSERC Undergraduate Research Award</b> - Worked with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/" target="_blank">Yu Sun</a> at the Robotics Institute of University of Toronto: Advanced Micro and Nanosystems Lab.</li>
            </ul>
            </div>
            <!--<h2>Research 	&#129302;</h2>-->
        </div>

        <!-- Featured Projects Section -->
        <div id="section-projects" class="content-section">
        <h2>Featured Projects</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">
                <papertitle>WATonoBus: A Brief Demo of Canada's First Autonomous 5G Shuttle</papertitle>
              </a>
              <br>
              [
              <a href="https://arxiv.org/pdf/2312.00938" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://uwaterloo.ca/watonobus/" target="_blank">Webpage</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=G0T5sKgqpLM&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=2&ab_channel=WaterlooEngineering" target="_blank">Video</a>               ]
              <p>Our team started the WATonoBus autonomous shuttle project in 2018 and has since developed 2 such fully equipped shuttles and is near completion of the third shuttle. The WATonoBus is a platform that contains fully in-house equipped hardware and software stack and has been approved as part of the Ministry of Transportation Ontario’s Autonomous Vehicle Pilot Program currently providing daily free and fully autonomous service to passengers at the University of Waterloo. The University of Waterloo's Ring Road is a 2.7 km curvy road with many intersections and pedestrian crossings that represent an urban driving environment with several pedestrians, cyclists, and vehicles. The WATonoBus project is different from prior project in that it is aimed to operate in all weather conditions including adverse rain, fog, and snow.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/node_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home" target="_blank">
                <papertitle>NODE: A Brief Demo of UAlberta's Autonomous Vehicle</papertitle>
              </a>
              <br>
              [
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home?authuser=0" target="_blank">Webpage</a> &nbsp/&nbsp
              <a href="https://youtu.be/bnfe1BZZiaw?t=52" target="_blank">Video</a>               ]
              <p>Since 2021, I have been leading efforts at the NODE lab to develop hardware and software stack on the NODE lab's autonomous Ford Escape vehicle. This platform has been central in working on several research projects covering RL, visual odometry, SLAM, object detection. I also supervise several PhD and Masters students at the lab.
              </p>
            </td>
          </tr>

        </tbody></table>
        </div>

        <!-- Selected Publications Section -->
        <div id="section-publications" class="content-section">
        <h2>Selected Publications</h2>
        <div style="margin-bottom: 20px;">
          <button class="filter-btn active" onclick="filterPublications('all', this)">All</button>
          <button class="filter-btn" onclick="filterPublications('world-modeling', this)">World Modeling</button>
          <button class="filter-btn" onclick="filterPublications('verification-guided', this)">Verification-Guided Learning</button>
          <button class="filter-btn" onclick="filterPublications('uncertainty-mitigation', this)">Uncertainty Mitigation</button>
          <button class="filter-btn" onclick="filterPublications('real-world-transfer', this)">Real-World Transfer</button>
        </div>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr class="pub-item" data-category="uncertainty-mitigation" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/uncap.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2510.12992" target="_blank">
                <papertitle>UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://d31003.github.io/" target="_blank">Po-han Li</a>,
              <a href="https://autonomy.oden.utexas.edu/profiles/kushagra-gupta" target="_blank">Kushagra Gupta</a>,
              <a href="https://www.linkedin.com/in/rohan-siva" target="_blank">Rohan Siva</a>,
              <a href="https://www.linkedin.com/in/milan-daniel" target="_blank">Daniel Milan</a>,
              <a href="https://www.linkedin.com/in/alexander-hogue" target="_blank">Alexander T. Hogue</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/sandeep-chinchali" target="_blank">Sandeep P. Chinchali</a>,
              <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/fridovich-keil" target="_blank">David Fridovich-Keil</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Zhangyang Wang</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Under Submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/abs/2510.12992" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://uncap-project.github.io/" target="_blank">Webpage</a>
               ]
              <p>We introduce UNCAP, a planning approach for connected autonomous vehicles that uses natural language communication to convey perception uncertainties. Our two-stage protocol selectively exchanges messages with relevant vehicles, reducing communication bandwidth by <strong>63%</strong> while increasing driving safety scores by <strong>31%</strong>.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/vln_zero.gif" alt="VLN-Zero demo" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2509.18592" target="_blank">
                <papertitle>VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang</a>,
              <a href="https://www.linkedin.com/in/rohan-siva" target="_blank">Rohan Siva</a>,
              <a href="https://www.linkedin.com/in/pranaysam" target="_blank">Pranay Samineni</a>,
              <a href="https://www.linkedin.com/in/milan-daniel" target="_blank">Daniel Milan</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Zhangyang Wang</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Under Submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/abs/2509.18592" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://vln-zero.github.io/" target="_blank">Webpage</a>
               ]
              <p>We present VLN-Zero, a zero-shot vision-language navigation framework that enables robots to follow natural language instructions in unseen environments without task-specific training. Our approach leverages rapid exploration and cache-enabled neurosymbolic planning to achieve effective zero-shot transfer in robot navigation tasks.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="verification-guided" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/repv.gif" alt="RepV animation" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2510.26935" target="_blank">
                <papertitle>RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification</papertitle>
              </a>
              <br>
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://www.linkedin.com/in/pranaysam" target="_blank">Pranay Samineni</a>,
              <a href="https://www.linkedin.com/in/rohan-siva" target="_blank">Rohan Siva</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Zhanyang Wang</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Under Submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/abs/2510.26935" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://repv-project.github.io/" target="_blank">Webpage</a>
               ]
              <p>We introduce RepV, a neurosymbolic verifier that learns a latent space where safe and unsafe plans are linearly separable. Starting with a seed set labeled by a model checker, RepV trains a projector embedding plans and rationales into a low-dimensional space. Our method improves compliance prediction accuracy by up to <strong>15%</strong> compared to baselines while adding fewer than <strong>0.2M parameters</strong>.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="verification-guided" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/joint_verification.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.14865" target="_blank">
                <papertitle>Joint Verification and Refinement of Language Models for Safety-Constrained Planning</papertitle>
              </a>
              <br>
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://oden.utexas.edu/people/directory/William-Ward/" target="_blank">William Ward</a>,
              <a href="https://www.zichaohu.com/" target="_blank">Zichao Hu</a>,
              <a href="https://www.cs.utexas.edu/people/faculty-researchers/joydeep-biswas" target="_blank">Joydeep Biswas</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Under Submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/abs/2410.14865" target="_blank">Paper</a>
              <!-- <a href="https://joint-verification.github.io/" target="_blank">Webpage</a> -->
               ]
              <p>We develop a method that converts generated robot programs into automaton-based representations and verifies them against safety specifications. We establish a theorem that any combination of verified programs also satisfies safety specifications, eliminating the need to verify complex composed programs. Our automated fine-tuning procedure increases the probability of generating specification-compliant programs by <strong>30%</strong>, with training time reduced by <strong>half</strong> compared to fine-tuning on full programs.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="uncertainty-mitigation" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/MLSys_2025_demos.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2411.01639" target="_blank">
                <papertitle>Know Where You're Uncertain When Planning with Multimodal Foundation Models: A Formal Framework</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang*</a>,
              <a href="https://www.linkedin.com/in/rohan-siva" target="_blank">Rohan Siva</a>,
              <a href="https://www.linkedin.com/in/milan-daniel" target="_blank">Daniel Milan</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>
              <br>
              <em>Conference on Machine Learning and Systems (MLSys), 2025</em>. 
              <br>
              <b style="color:red">One of 61 accepted papers!</b>
              <br>
              [
              <a href="https://arxiv.org/abs/2411.01639" target="_blank">Paper</a> &nbsp/&nbsp
              <!-- <a href="posters/MLSys-24.pdf" target="_blank">Poster</a> &nbsp/&nbsp -->
              <a href="https://uncertainty-in-planning.github.io/" target="_blank">Webpage</a> &nbsp/&nbsp
              <a href="https://drive.google.com/file/d/1n0fD2JajoGLRwpHaHfsR9MBKnRWK4H7r/view?usp=sharing" target="_blank">Video</a> &nbsp/&nbsp
              <a href="https://first-newt-mentally.ngrok-free.app/" target="_blank">Interactive Demo</a> &nbsp/&nbsp
              <a href="https://github.com/uncertainty-in-planning/uncertainty-in-planning.github.io?tab=readme-ov-file" target="_blank">Code</a>
               ]
              <p>We propose methods tailored to the unique properties of perception and decision-making: we use conformal prediction to calibrate perception uncertainty and introduce FMDP to quantify decision uncertainty, leveraging formal verification techniques for theoretical guarantees. Building on this quantification, we implement two targeted intervention mechanisms: an active sensing process and an automated refinement procedure improving its capability to meet task specifications. Empirical validation in real-world and simulated robotic tasks demonstrates that our uncertainty disentanglement framework reduces variability by up to <strong>40%</strong> and enhances task success rates by <strong>5%</strong> compared to baselines
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="verification-guided" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/o1-benchmark.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2409.19924" target="_blank">
                <papertitle>On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability</papertitle>
              </a>
              <br>
              <a href="https://www.kevin-ai.com/" target="_blank">Kevin Wang*</a>,
              <a href="https://ljb121002.github.io/" target="_blank">Junbo Li</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://scholar.google.com/citations?user=34s2YS0AAAAJ&hl=en" target="_blank">Yihan Xi</a>,
              <a href="https://www.cs.utexas.edu/~lqiang/" target="_blank">Qiang Liu</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>
              <br>
              <em>Conference on Neural Information Processing Systems (NeurIPS) Language Gamification Workshop, 2025</em>. 
              <br>
              <b style="color:red">Our paper reached 65k views on Youtube!</b>
              <br>
              [
              <a href="https://arxiv.org/abs/2409.19924" target="_blank">Paper</a> &nbsp/&nbsp
              <!-- <a href="posters/MLSys-24.pdf" target="_blank">Poster</a> &nbsp/&nbsp -->
              <a href="https://github.com/VITA-Group/o1-planning" target="_blank">Webpage and Code</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=FDrLWjLxBFw&amp;ab_channel=MatthewBerman" target="_blank">Video</a>
               ]
              <p>We evaluate the planning capabilities of OpenAI's o1 models across a variety of benchmark tasks, focusing on three key aspects: feasibility, optimality, and generalizability. Through empirical evaluations on constraint-heavy tasks (e.g., Barman, Tyreworld) and spatially complex environments (e.g., Termes, Floortile), we highlight o1-preview’s strengths in self-evaluation and constraint-following, while also identifying bottlenecks in decision-making and memory management, particularly in tasks requiring robust spatial reasoning. Our results reveal that o1-preview outperforms GPT-4 in adhering to task constraints and managing state transitions in structured environments. However, the model often generates suboptimal solutions with redundant actions and struggles to generalize effectively in spatially complex tasks.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="verification-guided" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/MLSys-24.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf" target="_blank">
                <papertitle>Fine-Tuning Language Models Using Formal Methods Feedback: A Use Case in Autonomous Systems</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://oden.utexas.edu/people/directory/Yunhao%20Yang/" target="_blank">Yunhao Yang*</a>,
              <a href="https://oden.utexas.edu/people/directory/Tyler-Ingebrand/" target="_blank">Tyler Ingebrand*</a>,
              <a href="https://oden.utexas.edu/people/directory/William-Ward/" target="_blank">William Ward</a>,
              <a href="https://oden.utexas.edu/people/directory/Steven-Carr/" target="_blank">Steven Carr</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>Conference on Machine Learning and Systems (MLSys), 2024</em>. 
              <br>
              <b style="color:red">One of only 37 accepted papers!</b>
              <br>
              [
              <a href="https://proceedings.mlsys.org/paper_files/paper/2024/file/b0131b6ee02a00b03fc3320176fec8f5-Paper-Conference.pdf" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="posters/MLSys-24.pdf" target="_blank">Poster</a> &nbsp/&nbsp
              <a href="slides/MLSys-24.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=-N6ugXfHaFI&ab_channel=AsimMunawar" target="_blank">Video</a>
               ]
              <p>We present a fully automated approach to fine-tune pre-trained language models for applications in autonomous systems, bridging the gap between generic knowledge and domain-specific requirements while reducing cost. The method synthesizes automaton-based controllers from pre-trained models guided by natural language task descriptions that are formally verified. Controllers with high compliance of the desired specifications receive higher ranks, guiding the iterative fine-tuning process. We provide quantitative evidence demonstrating an improvement in percentage of specifications satisfied from <strong>60%</strong> to <strong>90%</strong>.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/MM3DGS.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2404.00923" target="_blank">
                <papertitle>MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/lisong-codey-sun/" target="_blank">Lisong C. Sun*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://www.linkedin.com/in/jonathanliu88/" target="_blank">Jonathan C. Lu</a>,
              <a href="https://zhiwenfan.github.io/" target="_blank">Zhiwen Fan</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>,
              <a href="https://www.ae.utexas.edu/people/faculty/faculty-directory/humphreys" target="_blank">Todd E. Humphreys</a>,
              <a href="https://oden.utexas.edu/people/directory/ufuk--topcu/" target="_blank">Ufuk Topcu</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2024</em>.
              <br>
              <b style="color:red">Oral Pitch Finalist!</b>
              <br>
              [
              <a href="https://arxiv.org/pdf/2404.00923" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="posters/MM3DGS.pdf" target="_blank">Poster</a> &nbsp/&nbsp
              <a href="slides/MM3DGS.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=drf6UxehChE&ab_channel=NeelBhatt" target="_blank">Video</a> &nbsp/&nbsp
              <a href="https://vita-group.github.io/MM3DGS-SLAM/" target="_blank">Webpage</a>
               ]
              <p>Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. Experimental evaluation on several scenes shows a <strong>3x</strong> improvement in tracking and <strong>5%</strong> improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Comp4D_LLM.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2403.16993" target="_blank">
                <papertitle>Comp4D: LLM-Guided Compositional 4D Scene Generation</papertitle>
              </a>
              <br>
              <a href="https://ir1d.github.io/" target="_blank">Dejia Xu*</a>,
              <a href="https://scholar.google.com/citations?user=mrOHvI8AAAAJ&hl=en" target="_blank">Hanwen Liang*</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://alexhu.top/" target="_blank">Hezhen Hu</a>,
              <a href="https://scholar.google.com/citations?user=XcxDA14AAAAJ&hl=en/" target="_blank">Hanxue Liang</a>,
              <a href="https://www.ece.utoronto.ca/people/plataniotis-k-n/" target="_blank">Konstantinos N. Plataniotis</a>,
              <a href="https://www.ece.utexas.edu/people/faculty/atlas-wang" target="_blank">Atlas Wang</a>
              <br>
              <em>Under submission</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2403.16993" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=9q8SV1Xf_Xw&t=281s&ab_channel=DejiaXu" target="_blank">Video</a> &nbsp/&nbsp
              <a href="https://vita-group.github.io/Comp4D/" target="_blank">Webpage</a>
               ]
              <p>Our method, MM3DGS, addresses the limitations of prior neural radiance field-based representations by enabling faster rendering, scale awareness, and improved trajectory tracking. Our framework enables keyframe-based mapping and tracking utilizing loss functions that incorporate relative pose transformations from pre-integrated inertial measurements, depth estimates, and measures of photometric rendering quality. Experimental evaluation on several scenes shows a <strong>3x</strong> improvement in tracking and <strong>5%</strong> improvement in photometric rendering quality compared to the current 3DGS SLAM state-of-the-art, while allowing real-time rendering.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Frontiers-24.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.readcube.com/articles/10.3389/frobt.2024.1212070" target="_blank">
                <papertitle>A Survey on 3D Object Detection in Real-time for Autonomous Driving</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=nUF0J2IAAAAJ&hl=en" target="_blank">Marcelo Contreras</a>,
              <a href="https://in.linkedin.com/in/aayush-jain-406315202" target="_blank">Aayush Jain</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://scholar.google.co.in/citations?user=Bol-rwwAAAAJ&hl=en" target="_blank">Arunava Banerjee</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Frontiers in Robotics and AI, 2024</em>. 
              <br>
              [
              <a href="https://www.readcube.com/articles/10.3389/frobt.2024.1212070" target="_blank">Paper</a>
               ]
              <p>We provide a comprehensive survey and quantitative comparisons with state-of-the-art 3D object detection methodologies aiming to tackle varying weather conditions, multi-modality, multi-camera perspective, and their respective metrics associated to different difficulty categories. We identify several research gaps and potential future directions in visual-based 3D object detection approaches for autonomous driving.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpc-pf_its.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">
                <papertitle>MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Transactions on Intelligent Transportation Systems (T-ITS), 2023</em>. 
              <br>
              <b style="color:red">Outperforms 2<sup>nd</sup> and 3<sup>rd</sup> place on Waymo Motion Dataset (2021)</b>
              <br>
              [
              <a href="https://ieeexplore.ieee.org/document/10046400" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=56sD-qsREBY&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=3&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p>We propose MPC-PF, a model that embeds surrounding object and road map information in the form of a potential field to model agent-agent and agent-space interactions. We show the efficacy of our multi-object trajectory prediction method both qualitatively and quantitatively achieving state-of-the-art results on the Waymo Open Motion Dataset and other common urban driving scenarios.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/integrated-inertial-lidar.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">
                <papertitle>Integrated Inertial-LiDAR-Based Map Matching Localization for Varying Environments</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=vCYqMTIAAAAJ&hl=en" target="_blank">Xin Xia</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Transactions on Intelligent Vehicles (T-IV), 2023</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/10194984" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=7HaQQtZYQh4&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p> Leveraging a global navigation satellite system, inertial navigation system, and 3D LiDAR point clouds, a novel light point cloud map generation method, which only keeps the necessary point clouds (i.e., buildings and roads regardless of vegetation varying with seasonal change), is proposed. Thorough experiments in winter and summer confirm the advantages of integrating the proposed light point cloud map generation with the dead reckoning model in terms of accuracy and reduced computational complexity.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/SVO-23.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">
                <papertitle>A Stereo Visual Odometry Framework with Augmented Perception for Dynamic Urban Environments</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=nUF0J2IAAAAJ&hl=en" target="_blank">Marcelo Contreras</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2023</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/10421981" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="slides/SVO-23.pdf" target="_blank">Slides</a>
               ]
              <p> We propose a semantic-aware stereo visual odometry framework wherein feature extraction is performed over a static region-of-interest generated through object detection and instance segmentation on static street objects. Extensive real driving sequences in various dynamic urban environments with varying sequence lengths confirms excellent performance and computational efficiency attributed to using semantic-aware feature tracking.
              </p>
            </td>
          </tr>


          <tr class="pub-item" data-category="world-modeling" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:top">
              <img src="images/iros_poster.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046" target="_blank">
                <papertitle>MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=4DR7fqruLGo&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=1&ab_channel=NeelBhatt" target="_blank">Video</a>
               ]
              <p>Predicting object motion behaviour is a challenging but crucial task for safe decision-making and planning for an autonomous vehicle. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model produces accurate short and long-term predictions.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/augmented_case.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">
                <papertitle>Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ali-salimzade" target="_blank">Ali Salimzadeh</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>International Conference on Automation Science and Engineering (CASE), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="slides/case_presentation.pdf" target="_blank">Slides</a> &nbsp/&nbsp
              <a href="https://youtu.be/1bjuX6VuZjs" target="_blank">Video</a>
               ]
              <p>We develop a robust infrastructure-aided localization framework using only a single low-cost camera with a fisheye lens. To reduce the computational load, we use an ROI alongside estimated depth to re-project the robot pointcloud cluster with geometrical outlier detection. We use this position and depth information in an uncertainty-aware observer with adaptive covariance allocation and bounded estimation error to deal with position measurement noises at the limits of the field of view, and intermittent occlusion in dynamic environments. Moreover, we use a learning-based prediction model for input estimation based on a moving buffer of the robot position. Several experiments with occlusion and intermittent visual disruption/detection confirm effectiveness of the developed framework in re-initializing the estimation process after failure in the visual detection, and handling temporary data loss due to sensor faults or changes in lighting conditions.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infra_robotics_2.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://www.mdpi.com/2218-6581/11/4/82" target="_blank">
                <papertitle>Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://www.fzi.de/team/daniel-floegel/" target="_blank">Daniel Flögel</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Robotics Journal, 2022</em>. 
              <br>
              [
              <a href="https://www.mdpi.com/2218-6581/11/4/82" target="_blank">Paper</a>
               ]
              <p>A slip-aware localization framework is proposed for mobile robots experiencing wheel slip in dynamic environments. The framework fuses infrastructure-aided visual tracking data and proprioceptive sensory data from a skid-steer mobile robot to enhance accuracy and reduce variance of the estimated states. Covariance intersection is used to fuse the pose prediction and the visual thread, such that the updated estimate remains consistent. As confirmed by experiments on a skid-steer mobile robot, the designed localization framework addresses state estimation challenges for indoor/outdoor autonomous mobile robots which experience high-slip, uneven torque distribution at each wheel (by the motion planner), or occlusion when observed by an infrastructure-mounted camera.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpr_smcs.png" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417424026575?dgcid=coauthor" target="_blank">
                <papertitle>Soft Constrained Autonomous Vehicle Navigation using Gaussian Processes and Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=yv016ZMAAAAJ&hl=en" target="_blank">Bruno H. Groenner Barbosa</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>
              <br>
              <em>Expert Systems with Applications</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2101.06901" target="_blank">Paper</a>
               ]
              <p>We present a generic feature-based navigation framework for autonomous vehicles using a soft constrained Particle Filter. After obtaining features of mapped landmarks in instance-based segmented images acquired from a monocular camera, vehicle-to-landmark distances are predicted using Gaussian Process Regression (GPR) models in a mixture of experts approach. Experimental results confirm that the use of image segmentation features improves the vehicle-to-landmark distance prediction notably, and that the proposed soft constrained approach reliably localizes the vehicle even with reduced number of landmarks and noisy observations.
              </p>
            </td>
          </tr>

          <tr class="pub-item" data-category="real-world-transfer" onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhe_itsc.gif" alt="hpp" style="border-style: none" width="400">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">
                <papertitle>Real-time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ehsan-mohammadbagher" target="_blank">Ehsan Mohammadbagher*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi" target="_blank">Ehsan Hashemi</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/fidan" target="_blank">Baris Fidan</a>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo" target="_blank">Amir Khajepour</a>
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2020</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306" target="_blank">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=2ORorrOtIfE&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=4&ab_channel=NeelBhatt" target="_blank">Video</a>  
               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>

        </tbody></table>
        </div>

        <!-- Outreach Section -->
        <div id="section-outreach" class="content-section">
        <h2>Outreach</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/AEOP_REU_ULI_summer_2025.jpeg" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://autonomy.oden.utexas.edu/news/robotics-ai-and-programming-center-autonomy-summer-research-internship-programs" target="_blank">
                <papertitle>AEOP, Realtime Adaption REU, and NASA ULI Summer Internship Programs</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/news/robotics-ai-and-programming-center-autonomy-summer-research-internship-programs" target="_blank">Webpage</a>               ]
              <p>The Center for Autonomy at the Oden Institute hosted four outreach programs for undergraduates over the summer of 2025. Over the course of three eight-week programs, AEOP, Realtime Adaption REU and NASA's ULI, interns contributed to both software and hardware engineering tasks, supporting ground and legged platforms such as Clearpath Jackals, a Husky, and Unitree Go2 quadrupeds. Under the guidance of research mentor Dr. Christian Ellis and Ph.D. students, participants broke down complex research objectives into achievable tasks, ultimately developing software to work seamlessly with real robotic systems.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/Del_Valle_robotics_tour_2025.jpeg" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://autonomy.oden.utexas.edu/local-high-school-students-gain-hands-experience-through-robotics-tour" target="_blank">
                <papertitle>Del Valle High School Robotics Tour</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/local-high-school-students-gain-hands-experience-through-robotics-tour" target="_blank">Webpage</a>               ]
              <p>On February 25, 2025, the Center for Autonomy hosted upperclassmen in robotics and engineering programs at Del Valle High School, where 84.4% of students are economically disadvantaged. The students explored technological innovations at the frontiers of academia and industry, including tours of the Robotics Lab at Anna Hiss Gym and the Texas Advanced Computing Center's (TACC) Visualization Lab (VisLab) at the Oden Institute. Students learned about artificial intelligence for autonomous systems, including rovers, robotic arms, drones, and driverless cars.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/REACT_REU_2025.jpg" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://autonomy.oden.utexas.edu/center-autonomy-inspires-undergraduates-through-research-experience-program" target="_blank">
                <papertitle>REACT Research Experience for Undergraduates (REU)</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/center-autonomy-inspires-undergraduates-through-research-experience-program" target="_blank">Webpage</a>               ]
              <p>The Center for Autonomy hosted the Introductory Research Experience in Autonomy and Control Technologies (REACT) Research Experience for Undergraduates (REU) from July 28 to August 10, 2025. REACT is a collaborative effort between the University of Texas at Austin, the University of New Mexico, and Hampton University. The two-week research program is designed for undergraduate students majoring in engineering disciplines such as aerospace, computer, electrical, and mechanical, as well as computer science, mathematics, and statistics. Through the program, students engage in cutting-edge research in world-class laboratories at UT Austin, collaborate with fellow researchers, attend workshops, and tour industry and research facilities.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TACC_outreach_combined.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://public.cyber.mil/gencyber/" target="_blank">
                <papertitle>GenCyber Back@TACC Coding and Cybersecurity Camp / TACC and Center for Autonomy</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/news/center-autonomy-volunteers-gencyber-tacc-camps" target="_blank">Webpage</a>               ]
              <p>In December of 2023, the Texas Advanced Computing Center (TACC) hosted two engaging cybersecurity events, "GenCyber Back@TACC" and "Level UP GenCyber Back@TACC," both aimed at educating high school students in various aspects of coding and cybersecurity. I contributed by motivating students envision a future in both cybersecurity and the broader selection of STEM fields. I also helped by facilitating the learning process, answering questions, and engaging high school students in discussions about their personal experiences from their academic journeys in STEM, thereby encouraging students to consider future studies and careers.
              </p>
            </td>
          </tr>


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/High_school_FRC_outreach.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://autonomy.oden.utexas.edu/learning-together-del-valle-students-experience-center-autonomy" target="_blank">
                <papertitle>Del Valle High School FRC Team Tours / TACC and Center for Autonomy</papertitle>
              </a>
              <br>
              [
              <a href="https://autonomy.oden.utexas.edu/learning-together-del-valle-students-experience-center-autonomy" target="_blank">Webpage</a>               ]
              <p>This event, hosted by Center for Autonomy, invited students from the Del Valle Independent School District (Travis County, TX), out of which 84.4% are economically disadvantaged. I presented to the students a high-level overview of what academic research looks like through my research on multi-modal 3D Guassian splatting for simultaneous localization and mapping and its implementation on the Clearpath jackal robots. I used TACC’s high resolution screens to deliver my presentation in an interactive manner. I shared my undergrad, internship, PhD, and postdoc experiences with the students and gave them a personal view of how each one led to the next career step so that they could understand potential career trajectories. I motivated the students to ask questions and answered their questions ranging from how I got into robotics through First Robotics Challenge (FRC) – their school participates in the same competition, what I do not like about industry, and failures through my career journey.
              </p>
            </td>
          </tr>

        </tbody></table>
        </div>

        <!-- Media Coverage Section -->
        <div id="section-media" class="content-section">
        <h2>Media Coverage</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td colspan="2" style="padding:20px 20px 10px 20px;">
              <h3 style="margin:0; font-size:18px; font-weight:bold;">Interviews & Spotlights</h3>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ualberta_spotlight.png" alt="U of A Innovator Spotlight" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.ualberta.ca/the-quad/2023/06/innovator-spotlight-neel-bhatt.html" target="_blank">
                <papertitle>Innovator Spotlight – University of Alberta</papertitle>
              </a>
              <br>
              [
              <a href="https://www.ualberta.ca/the-quad/2023/06/innovator-spotlight-neel-bhatt.html" target="_blank">Article</a> &nbsp/&nbsp
              <a href="https://www.linkedin.com/posts/university-of-alberta_ai-innovator-neel-bhatt-activity-7084575724824629248-NH1-" target="_blank">Video Feature</a>
               ]
              <p>Featured interview highlighting research contributions and innovations in autonomous systems and AI at the University of Alberta.
              </p>
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding:30px 20px 10px 20px;">
              <h3 style="margin:0; font-size:18px; font-weight:bold;">WATonoBus / Autonomous Shuttle in the News</h3>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_waterloo_news.png" alt="WATonoBus Media Coverage" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">
                <papertitle>WATonoBus in the News – Canada's First Driverless Autonomous 5G Shuttle</papertitle>
              </a>
              <br>
              [
              <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless" target="_blank">Waterloo News</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=L-2PIEYpzuQ" target="_blank">Demo Video</a> &nbsp/&nbsp
              <a href="https://kitchener.citynews.ca/2021/06/24/uw-rolls-out-first-autonomous-bus-in-canada-3905040/" target="_blank">CityNews</a> &nbsp/&nbsp
              <a href="https://www.ctvnews.ca/kitchener/article/university-of-waterloo-researchers-create-self-driving-shuttle-bus/" target="_blank">CTV News</a> &nbsp/&nbsp
              <a href="https://www.cbc.ca/news/canada/kitchener-waterloo/driverless-shuttle-university-of-waterloo-watonobus-1.6078724" target="_blank">CBC News</a>
               ]
              <p>Extensive media coverage of Canada's first autonomous 5G shuttle bus project, including features in major news outlets covering the launch, development, and impact of the WATonoBus autonomous transportation system at University of Waterloo.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_seeed.jpg" alt="Seeed Studio WATonoBus" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.seeedstudio.com/blog/2023/11/27/autonomous-shuttle-bus-at-university-of-waterloo-ai-powered-driving-environmental-and-traffic-perception/" target="_blank">
                <papertitle>Seeed Studio Feature – AI-powered Driving Environmental and Traffic Perception</papertitle>
              </a>
              <br>
              [
              <a href="https://www.seeedstudio.com/blog/2023/11/27/autonomous-shuttle-bus-at-university-of-waterloo-ai-powered-driving-environmental-and-traffic-perception/" target="_blank">Article</a>
               ]
              <p>Technical feature article by Seeed Studio covering the WATonoBus project's AI-powered environmental perception, traffic analysis, and autonomous driving capabilities using NVIDIA Jetson Orin NX edge computing.
              </p>
            </td>
          </tr>

          <tr>
            <td colspan="2" style="padding:30px 20px 10px 20px;">
              <h3 style="margin:0; font-size:18px; font-weight:bold;">Neurosymbolic AI Research</h3>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurosymbolic_ai_colorado.png" alt="Neurosymbolic AI Research Coverage" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.colorado.edu/today/2025/06/05/democratize-ai-make-it-work-more-human-brain" target="_blank">
                <papertitle>Neurosymbolic AI Research – Democratizing Brain-Inspired AI</papertitle>
              </a>
              <br>
              [
              <a href="https://www.colorado.edu/today/2025/06/05/democratize-ai-make-it-work-more-human-brain" target="_blank">Colorado Boulder</a> &nbsp/&nbsp
              <a href="https://autonomy.oden.utexas.edu/news/ut-austin-researchers-help-chart-path-more-accessible-brain-inspired-ai" target="_blank">UT Austin</a> &nbsp/&nbsp
              <a href="https://www.eurekalert.org/news-releases/1084289" target="_blank">EurekAlert</a>
               ]
              <p>Media coverage of research on democratizing AI and making it work more like the human brain, featuring neurosymbolic AI approaches and brain-inspired computing. Includes features from University of Colorado Boulder, UT Austin Center for Autonomy, and scientific press releases.
              </p>
            </td>
          </tr>

        </tbody></table>
        </div>

      </td>
    </tr>
  </table>
  Template adopted from <a href="https://jonbarron.info/" target="_blank">here</a>.
  <script>
    // Section navigation function - scrolls to section
    function scrollToSection(sectionId, button) {
      // Remove active class from all tab buttons
      const tabButtons = document.querySelectorAll('.tab-btn');
      tabButtons.forEach(btn => {
        btn.classList.remove('active');
      });
      
      // Add active class to clicked button
      if (button) {
        button.classList.add('active');
      }
      
      // Scroll to the section
      const section = document.getElementById(sectionId);
      if (section) {
        // Get the offset position of the section
        const offsetTop = section.offsetTop - 20; // 20px offset for better visibility
        window.scrollTo({
          top: offsetTop,
          behavior: 'smooth'
        });
      }
    }

    // Publication filtering function - must be global for onclick handlers
    function filterPublications(category, button) {
      // Remove active class from all buttons
      const buttons = document.querySelectorAll('.filter-btn');
      buttons.forEach(btn => btn.classList.remove('active'));
      
      // Add active class to clicked button
      button.classList.add('active');
      
      // Get all publication items
      const items = document.querySelectorAll('.pub-item');
      
      // Show/hide items based on category
      items.forEach(item => {
        if (category === 'all') {
          item.style.display = 'table-row';
        } else {
          // Get categories as space-separated string and split into array
          const itemCategories = item.getAttribute('data-category');
          if (itemCategories) {
            const categoriesArray = itemCategories.split(' ').filter(c => c.length > 0);
            // Show if the publication belongs to the selected category
            if (categoriesArray.includes(category)) {
              item.style.display = 'table-row';
            } else {
              item.style.display = 'none';
            }
          } else {
            item.style.display = 'none';
          }
        }
      });
    }

    document.addEventListener('DOMContentLoaded', function () {
      if (window.particlesJS) {
        var particlesInstance = particlesJS('network-bg', {
          particles: {
            number: {
              value: 55,
              density: {
                enable: true,
                value_area: 900
              }
            },
            color: { value: '#9c9c9c' },
            shape: { type: 'circle' },
            opacity: {
              value: 0.45,
              random: true
            },
            size: {
              value: 3,
              random: true
            },
            line_linked: {
              enable: true,
              distance: 160,
              color: '#b7b7b7',
              opacity: 0.5,
              width: 1
            },
            move: {
              enable: true,
              speed: 1.1,
              direction: 'none',
              out_mode: 'out'
            }
          },
          interactivity: {
            detect_on: 'canvas',
            events: {
              onhover: { enable: true, mode: 'grab' },
              onclick: { enable: false },
              resize: true
            },
            modes: {
              grab: {
                distance: 200,
                line_linked: { opacity: 0.7 }
              }
            }
          },
          retina_detect: true
        });

        // Add mouse tracking - all particles connect to cursor
        setTimeout(function() {
          var particlesCanvas = document.querySelector('#network-bg canvas');
          if (!particlesCanvas) return;
          
          // Access particles.js instance
          var pJS = null;
          if (particlesInstance && particlesInstance.pJSDom && particlesInstance.pJSDom[0] && particlesInstance.pJSDom[0].pJS) {
            pJS = particlesInstance.pJSDom[0].pJS;
          } else {
            var pJSDom = document.querySelector('#network-bg');
            if (pJSDom && pJSDom.pJSDom && pJSDom.pJSDom[0] && pJSDom.pJSDom[0].pJS) {
              pJS = pJSDom.pJSDom[0].pJS;
            }
          }
          
          if (!pJS || !pJS.fn || !pJS.fn.draw) return;
          
          var mousePos = { x: null, y: null };
          var maxDistance = 500;
          
          // Track mouse position - convert screen coordinates to canvas coordinates
          function updateMousePos(e) {
            // Always get fresh bounding rect
            var rect = particlesCanvas.getBoundingClientRect();
            
            // Get canvas internal dimensions (actual pixel size)
            var canvasWidth = particlesCanvas.width;
            var canvasHeight = particlesCanvas.height;
            
            // Get canvas display dimensions (CSS size)
            var displayWidth = rect.width;
            var displayHeight = rect.height;
            
            // Fallback to window size if rect is invalid
            if (!displayWidth || !displayHeight) {
              displayWidth = window.innerWidth;
              displayHeight = window.innerHeight;
            }
            
            // Handle edge cases
            if (canvasWidth === 0 || canvasHeight === 0) {
              return;
            }
            
            // Calculate scale factors
            var scaleX = canvasWidth / displayWidth;
            var scaleY = canvasHeight / displayHeight;
            
            // Get mouse position relative to viewport
            var viewportX = e.clientX;
            var viewportY = e.clientY;
            
            // Convert to position relative to canvas element
            var canvasX = viewportX - rect.left;
            var canvasY = viewportY - rect.top;
            
            // Clamp to canvas display bounds
            canvasX = Math.max(0, Math.min(canvasX, displayWidth));
            canvasY = Math.max(0, Math.min(canvasY, displayHeight));
            
            // Convert to canvas internal coordinate space
            mousePos.x = canvasX * scaleX;
            mousePos.y = canvasY * scaleY;
          }
          
          // Listen to mouse movement on entire document (works even over content)
          document.addEventListener('mousemove', updateMousePos);
          window.addEventListener('mouseleave', function() {
            mousePos.x = null;
            mousePos.y = null;
          });
          
          // Store original draw function
          var originalDraw = pJS.fn.draw.bind(pJS.fn);
          
          // Override draw to add cursor connections after particles are drawn
          pJS.fn.draw = function() {
            // Draw particles first
            originalDraw();
            
            // Get current mouse position (use our tracked position or particles.js internal position)
            var currentMouseX = mousePos.x;
            var currentMouseY = mousePos.y;
            
            // Try to use particles.js internal mouse position as fallback
            if ((currentMouseX === null || currentMouseY === null) && pJS.interactivity && pJS.interactivity.mouse) {
              currentMouseX = pJS.interactivity.mouse.pos_x;
              currentMouseY = pJS.interactivity.mouse.pos_y;
            }
            
            // Then draw connections from ALL particles to cursor
            if (currentMouseX !== null && currentMouseY !== null && pJS.particles && pJS.particles.array && pJS.canvas && pJS.canvas.ctx) {
              var ctx = pJS.canvas.ctx;
              var lineColor = pJS.particles.line_linked.color || '#b7b7b7';
              var lineWidth = 1;
              
              ctx.save();
              ctx.strokeStyle = lineColor;
              ctx.lineWidth = lineWidth;
              
              // Draw line from EVERY particle to cursor (within maxDistance)
              pJS.particles.array.forEach(function(particle) {
                var dx = currentMouseX - particle.x;
                var dy = currentMouseY - particle.y;
                var distance = Math.sqrt(dx * dx + dy * dy);
                
                // Connect all particles within range, fade based on distance
                if (distance > 0 && distance < maxDistance) {
                  var opacity = 0.5 * (1 - distance / maxDistance);
                  ctx.globalAlpha = Math.max(0.1, opacity);
                  ctx.beginPath();
                  ctx.moveTo(particle.x, particle.y);
                  ctx.lineTo(currentMouseX, currentMouseY);
                  ctx.stroke();
                }
              });
              
              ctx.restore();
            }
          };
        }, 1000);
      }
    });
  </script>
</body>

</html>
