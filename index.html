<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Neel P. Bhatt</title>
  
  <meta name="author" content="Neel P. Bhatt">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/uw_logo.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Neel P. Bhatt</name>
              </p>
              <p>I am currently a visiting researcher at the <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home">NODE Lab</a> at University of Alberta and recently completed my PhD from University of Waterloo. My research is centered at the intersection of perception, state estimation, prediction, and decision making for autonomous driving.</p>
              <p>During my PhD at University of Waterloo, I led efforts on the <a href="https://uwaterloo.ca/watonobus/">WATonoBus project</a> at <a href="https://uwaterloo.ca/mechatronic-vehicle-systems-lab/">MVS Lab</a> working on software and algorithmic development of perception and prediction modules required for <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless">Canada’s first autonomous shuttle bus</a> approved via the ministry’s autonomous vehicle pilot.</p>
              <p> During my undergraduate studies at University of Toronto, I worked with Professor <a href="http://web.mit.edu/cocosci/josh.html">Yu Sun</a> on micro and nano robotics.</p>
              <p style="text-align:center">
                    [
                <a href="mailto:npbhatt@uwaterloo.ca">Email</a> &nbsp/&nbsp
                <a href="https://uwaterloo.ca/scholar/npbhatt/home">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=S8ofWDUAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/neelbhattportfolio/">LinkedIn</a>
]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_v3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_v3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                <li><b>June 2023</b>: <b>Featured Video</b> - I was featured on the <a href="https://www.ualberta.ca/research/our-research/artificial-intelligence.html">homepage</a> of Artificial Intelligence Research and Innovation at University of Alberta via a <a href="https://youtu.be/bnfe1BZZiaw?t=52">promo video</a>.</li>
                <li><b>June 2023</b>: <b>Workshop</b> - Co-organizing a workshop <a href="https://sites.google.com/view/uofatutorialiv23/home">Reliable State Estimation and Distributed Controls in Intelligent Vehicular Networks</a> at IV 2023.</li>
                <li><b>May 2023</b>: <b style="color:red">Defended PhD Thesis</b> - My thesis work is out on <a href="https://uwspace.uwaterloo.ca/handle/10012/19210">UW Space</a>.</li> 
                <li><b>May 2023</b>: <b>Paper Published @ T-ITS 2023</b> - Our work <a href="https://ieeexplore.ieee.org/document/10046400">MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</a> was accepted to T-ITS 2023.</li> 
                <li><b>April 2023</b>: <b>Guest Lecture</b> - Gave a guest lecture on WATonoBus ‑ Algorithms and Software Structure for an All Weather Shuttle for MECE788 at University of Waterloo.</li> 
                <li><b>October 2022</b>: <b>Paper Published @ IROS 2022</b> - I will be presenting our work <a href="https://sites.google.com/view/neverendingrl/home?authuser=0">MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</a> at IROS 2022 in Kyoto.</li>
                <li><b>August 2022</b>: <b>Paper Published @ CASE 2022</b> - Our work on <a href="https://www.mdpi.com/1782890">Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</a> is accepted to CASE 2022.</li>
                <li><b>August 2022</b>: <b>Paper Published @ Robotics 2022</b> - Our work on <a href="https://ieeexplore.ieee.org/abstract/document/9926595">Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</a> is accepted to Robotics 2022.</li>
                <li><b>June 2021</b>: <b>WATonoBus Project</b> - The <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless">autonomous shuttle project</a> I am leading at MVS Lab was given approval as part of the ministry's autonomous vehicle pilot.</li>
                <li><b>September 2020</b>: <b>Paper Published @ ITSC 2020</b> - I will be presenting our work <a href="https://ieeexplore.ieee.org/abstract/document/9294306">Real‑time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</a> at ITSC 2020 in Greece.</li>
                <li><b>September 2020</b>: <b style="color:red">Engineering Excellence Doctoral Fellowship</b> - I was awarded with the <a href="https://uwaterloo.ca/graduate-studies-postdoctoral-affairs/awards/engineering-excellence-masters-and-doctoral-fellowships-eemf#:~:text=Value%20description%3A-,EEDF,-%2D%20%24120%2C000%20paid%20across">EEDF</a> for my PhD work.</li>
                <li><b>May 2019</b>: <b>Internship</b> - Joined <a href="https://www.gm.com/stories/100-years-research-development">GM</a> at their Global R&D center as an AV Software Engineering Intern working on Deep Learning-based State Estimation.</li>
               <li><b>May 2018</b>: <b>Completed BASc</b> - I finished my Bachelor's program at University of Toronto and was admitted to a direct PhD program at University of Waterloo.</li>
                <li><b>May 2017</b>: <b>Internship</b> funded with <b style="color:red">NSERC Industrial Experience Award</b> - Joined <a href="https://clearpathrobotics.com/">Clearpath Robotics</a> as an R&D Appications Engineering Intern working on ROS projects.</li>
                <li><b>May 2016</b>: <b>Research Internship</b> funded with <b style="color:red">NSERC Undergraduate Research Award</b> - Worked with Professor <a href="https://www.mie.utoronto.ca/faculty_staff/sun/">Yu Sun</a> at the Robotics Institute of University of Toronto: Advanced Micro and Nanosystems Lab.</li>
            </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Publications</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <!--
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/todo.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="todo_link">
                <papertitle>TODO_paper_title</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              TODO,
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>TODO conference venue</em>. 
              <br>
              <p>TODO tldr</p>
            </td>
          </tr>
                -->


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpc-pf_its.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10046400">
                <papertitle>MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Transactions on Intelligent Transportation Systems (T-ITS), 2023</em>. 
              <br>
              <b style="color:red">Outperforms 2<sup>nd</sup> and 3<sup>rd</sup> place on Waymo Motion Dataset (2021)</b>
              <br>
              [
              <a href="https://ieeexplore.ieee.org/document/10046400">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=56sD-qsREBY&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=3&ab_channel=NeelBhatt">Video</a>
               ]
              <p>We propose MPC-PF, a model that embeds surrounding object and road map information in the form of a potential field to model agent-agent and agent-space interactions. We show the efficacy of our multi-object trajectory prediction method both qualitatively and quantitatively achieving state-of-the-art results on the Waymo Open Motion Dataset and also on other common urban driving scenarios where social norms must be followed.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros_poster.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">
                <papertitle>MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=4DR7fqruLGo&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=1&ab_channel=NeelBhatt">Video</a>
               ]
              <p>Predicting object motion behaviour is a challenging but crucial task for safe decision making and path planning for an autonomous vehicle. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model is capable of producing accurate predictions for short and long term timesteps and demonstrate significance of model architecture via an ablation study.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/augmented_case.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">
                <papertitle>Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ali-salimzade">Ali Salimzadeh</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>International Conference on Automation Science and Engineering (CASE), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">Paper</a>
               ]
              <p>We develop a robust infrastructure-aided localization framework using only a single low-cost camera with a fisheye lens. To reduce the computational load, we use an ROI alongside estimated depth to re-project the robot pointcloud cluster with geometrical outlier detection. We use this position and depth information in an uncertainty-aware observer with adaptive covariance allocation and bounded estimation error to deal with position measurement noises at the limits of the field of view, and intermittent occlusion in dynamic environments. Moreover, the aforementioned motion model, uses a learning-based prediction model for input estimation based on a moving buffer of the robot position. Several experiments with occlusion and intermittent visual disruption/detection confirm effectiveness of the developed framework in re-initializing the estimation process after failure in the visual detection, and handling temporary data loss due to sensor faults or changes in lighting conditions.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infra_robotics_2.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/2218-6581/11/4/82">
                <papertitle>Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://www.fzi.de/team/daniel-floegel/">Daniel Flögel</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Robotics Journal, 2022</em>. 
              <br>
              [
              <a href="https://www.mdpi.com/2218-6581/11/4/82">Paper</a>
               ]
              <p>A slip-aware localization framework is proposed for mobile robots experiencing wheel slip in dynamic environments. The framework fuses infrastructure-aided visual tracking data and proprioceptive sensory data from a skid-steer mobile robot to enhance accuracy and reduce variance of the estimated states. Covariance intersection is used to fuse the pose prediction and the visual thread, such that the updated estimate remains consistent. As confirmed by experiments on a skid-steer mobile robot, the designed localization framework addresses state estimation challenges for indoor/outdoor autonomous mobile robots which experience high-slip, uneven torque distribution at each wheel (by the motion planner), or occlusion when observed by an infrastructure-mounted camera.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpr_smcs.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.06901">
                <papertitle>Soft Constrained Autonomous Vehicle Navigation using Gaussian Processes and Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=yv016ZMAAAAJ&hl=en">Bruno H. Groenner Barbosa</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Submitted to Transactions on Systems, Man, and Cybernetics: Systems (SMCS)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2101.06901">Paper</a>
               ]
              <p>We present a generic feature-based navigation framework for autonomous vehicles using a soft constrained Particle Filter. After obtaining features of mapped landmarks in instance-based segmented images acquired from a monocular camera, vehicle-to-landmark distances are predicted using Gaussian Process Regression (GPR) models in a mixture of experts approach. Experimental results confirm that the use of image segmentation features improves the vehicle-to-landmark distance prediction notably, and that the proposed soft constrained approach reliably localizes the vehicle even with reduced number of landmarks and noisy observations.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhe_itsc.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>Real-time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ehsan-mohammadbagher">Ehsan Mohammadbagher*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2020</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=2ORorrOtIfE&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=4&ab_channel=NeelBhatt">Video</a>  
               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>
               
        </tbody></table>

        <h2>Featured Projects</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/watonobus_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>WATonoBus: A Brief Demo of Canada's First Autonomous 5G Shuttle</papertitle>
              </a>
              <br>
              [
              <a href="https://uwaterloo.ca/watonobus/">Webpage</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=G0T5sKgqpLM&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=2&ab_channel=WaterlooEngineering">Video</a>               ]
              <p>Our team started the WATonoBus autonomous shuttle project in 2018 and has since developed 2 such fully equipped shuttles and is near completion of the third shuttle. The WATonoBus is a platform that contains fully in-house equipped hardware and software stack and has been approved as part of the Ministry of Transportation Ontario’s Autonomous Vehicle Pilot Program currently providing daily free and fully autonomous service to passengers at the University of Waterloo. The University of Waterloo's Ring Road is a 2.7 km curvy road with many intersections and pedestrian crossings that represent an urban driving environment with several pedestrians, cyclists, and vehicles. The WATonoBus project is different from prior project in that it is aimed to operate in all weather conditions including adverse rain, fog, and snow.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/node_overview.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>NODE: A Brief Demo of UAlberta's Autonomous Vehicle</papertitle>
              </a>
              <br>
              [
              <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home?authuser=0">Webpage</a> &nbsp/&nbsp
              <a href="https://youtu.be/bnfe1BZZiaw?t=52">Video</a>               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>

        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>
