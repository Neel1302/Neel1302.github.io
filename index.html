<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Neel P. Bhatt</title>
  
  <meta name="author" content="Neel P. Bhatt">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/uw_logo.png">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Neel P. Bhatt</name>
              </p>
              <p>I am currently a visiting researcher at the <a href="https://sites.google.com/view/ehsan-hashemi-uwaterloo/home">NODE Lab</a> at University of Alberta and recently completed my PhD from University of Waterloo. My research is centered at the intersection of perception, state estimation, prediction, and decision making for autonomous driving.</p>
              <p>During my PhD at University of Waterloo, I led efforts on the <a href="https://uwaterloo.ca/watonobus/">WATonoBus project</a> at <a href="https://uwaterloo.ca/mechatronic-vehicle-systems-lab/">MVS Lab</a> working on software and algorithmic development of perception and prediction modules required for <a href="https://uwaterloo.ca/news/media/university-waterloo-launches-canadas-first-driverless">Canada’s first autonomous shuttle bus</a> approved via the ministry’s autonomous vehicle pilot.</p>
              <p> During my undergraduate studies at University of Toronto, I worked with Professor <a href="http://web.mit.edu/cocosci/josh.html">Yu Sun</a> on micro and nano robotics.</p>
              <p style="text-align:center">
                    [
                <a href="mailto:npbhatt@uwaterloo.ca">Email</a> &nbsp/&nbsp
                <a href="https://uwaterloo.ca/scholar/npbhatt/home">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=S8ofWDUAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/neelbhattportfolio/">LinkedIn</a>
]
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_pic_v3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_pic_v3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <!-- <h2>Background</h2> -->
        <p> 
        <!-- with <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a> and <a href="https://www.afaust.info/">Aleksandra Faust</a> in similar areas. --> 
        <!-- I was fortunate to also work on consumer privacy rights and legislation with folks from <a href="https://www.law.georgetown.edu/privacy-technology-center">Georgetown's Center on Privacy and Technology.</a></p> -->
              </p>
              <p>
              </p>
        <h2>News</h2>

            <ul>
                <li><b>May 2023</b>: <b style="color:red">Oral @ ICLR 2022</b> - Our work <a href="https://arxiv.org/pdf/2203.11370.pdf">Language modeling via stochastic processes</a> was accepted to ICLR 2022 as an oral (<1.6%).</li> 
                <li><b>August 2021</b>: <b>Paper @ EMLNLP 2021</b> - Our work <a href="https://arxiv.org/pdf/2110.05422.pdf">Calibrate your listeners! Robust communication-based training for pragmatic speakers</a> was accepted to Findings of EMNLP 2021.</li> 
                <li><b>April 2021</b>: <b>Blog</b> - Published a <a href="https://ai.googleblog.com/2021/04/model-based-rl-for-decentralized-multi.html">Google AI Blog post</a> on our multirobot collaboration work, <a href="https://arxiv.org/abs/2003.06906"> Model-based Reinforcement Learning for Multiagent Goal Alignment</a>.</li> 
                <li><b>January 2021</b>: <b>Workshop</b> - Co-organizing the workshop <a href="https://sites.google.com/view/neverendingrl/home?authuser=0">A Roadmap to Never-Ending Reinforcement Learning</a> at ICLR 2021. Please consider submitting your work!</li>
                <li><b>December 2020</b>: <b style="color:red">Best paper award</b> - Awarded the <b><a href="https://www.cooperativeai.com/home">NeurIPS 2020 Cooperative AI workshop</a> best paper award</b> for our work, <a href="https://arxiv.org/abs/2003.11778">"Too many cooks: Bayesian inference for coordinating multi-agent collaboration"</a>!
                <li><b>November 2020</b>: <b style="color:red">Code release</b> - Excited to opensource `gym-cooking` (<a href="https://github.com/rosewang2008/gym-cooking">github link</a>), a novel multi-agent Gym environment. Go try it out! 
                <li><b>October 2020</b>: <b style="color:red">New paper</b> - Paper "Model-based Reinforcement Learning for Multiagent Goal Alignment" in collaboration with Google Brain Robotics was accepted to CoRL 2020.
                <li><b>July 2020</b>: <b>Workshop</b> - Co-organizing the <a href="https://sites.google.com/view/resistance-ai-neurips-20/home">Resistance AI workshop</a> at NeurIPS 2020.</li>
                <li><b>July 2020</b>: <b>Workshop</b> - Co-leading the <a href="https://wimlworkshop.org/icml2020/">ICML 2020 Women in Machine Learning</a> session on continual reinforcement learning.</li>
                <li><b>June 2020</b>: <b style="color:red">Best paper award</b> - Awarded the <b><a href="https://cognitivesciencesociety.org/cogsci-2020/">CogSci 2020</a> best paper award</b> in computational modeling for our work, <a href="https://arxiv.org/abs/2003.11778">"Too many cooks: Coordinating multi-agent collaboration through inverse planning"</a>.</li>
                <li><b>April 2020</b>: <b style="color:red">Fellowship award</b> - Received the <a href="https://www.nsfgrfp.org/">NSF Graduate Research Fellowship</a>---excited for the research journey ahead!</li>
                <li><b>March 2020</b>: <b>Internship</b> - Joined Google Brain working with <a href="https://research.google/people/AleksandraFaust/">Aleksandra Faust</a> and <a href="https://webdocs.cs.ualberta.ca/~dale/">Dale Schuurmans</a>.</li>
                <li><b>September 2019</b>: <b>Internship</b> - Joined <a href="https://research.google/teams/brain/robotics/">Google Brain Robotics</a> interning on the Task and Motion Planning team.</li>
            </ul>
            <!--<h2>Research 	&#129302;</h2>-->
        <h2>Research</h2>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <!--
          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/todo.png" alt="hpp" style="border-style: none" width="200">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="todo_link">
                <papertitle>TODO_paper_title</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              TODO,
              <a href="https://cocolab.stanford.edu/ndg">Noah Goodman</a>
              <br>
              <em>TODO conference venue</em>. 
              <br>
              <p>TODO tldr</p>
            </td>
          </tr>
                -->


          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mpc-pf_its.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/document/10046400">
                <papertitle>MPC-PF: Socially and Spatially Aware Object Trajectory Prediction for Autonomous Driving Systems Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Transactions on Intelligent Transportation Systems (T-ITS), 2023</em>. 
              <br>
              <b style="color:red">Outperforms 2<sup>nd</sup> and 3<sup>rd</sup> place on Waymo Motion Dataset (2021)</b>
              <br>
              [
              <a href="https://ieeexplore.ieee.org/document/10046400">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=56sD-qsREBY&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=3&ab_channel=NeelBhatt">Video</a>
               ]
              <p>We propose MPC-PF, a model that embeds surrounding object and road map information in the form of a potential field to model agent-agent and agent-space interactions. We show the efficacy of our multi-object trajectory prediction method both qualitatively and quantitatively achieving state-of-the-art results on the Waymo Open Motion Dataset and also on other common urban driving scenarios where social norms must be followed.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros_poster.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">
                <papertitle>MPC-PF: Social Interaction Aware Trajectory Prediction of Dynamic Objects for Autonomous Driving Using Potential Fields</papertitle>
              </a>
              <br>
              <strong>Neel P. Bhatt</strong>,
              <a href="https://uwaterloo.ca/mechanical-mechatronics-engineering/profile/akhajepo">Amir Khajepour</a>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9981046">Paper</a> &nbsp/&nbsp
              <a href="https://www.youtube.com/watch?v=4DR7fqruLGo&list=PLQG3T3nkAnJ8qxJz8-jpqMEWN5JEh0Wt3&index=1&ab_channel=NeelBhatt">Video</a>
               ]
              <p>Predicting object motion behaviour is a challenging but crucial task for safe decision making and path planning for an autonomous vehicle. We tackle this problem by introducing MPC-PF: a novel potential field-based trajectory predictor that incorporates social interaction and is able to tradeoff between inherent model biases across the prediction horizon. Through evaluation on a variety of common urban driving scenarios, we show that our model is capable of producing accurate predictions for short and long term timesteps and demonstrate significance of model architecture via an ablation study.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/augmented_case.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">
                <papertitle>Augmented Visual Localization Using a Monocular Camera for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ali-salimzade">Ali Salimzadeh</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>International Conference on Automation Science and Engineering (CASE), 2022</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9926595">Paper</a>
               ]
              <p>We develop a robust infrastructure-aided localization framework using only a single low-cost camera with a fisheye lens. To reduce the computational load, we use an ROI alongside estimated depth to re-project the robot pointcloud cluster with geometrical outlier detection. We use this position and depth information in an uncertainty-aware observer with adaptive covariance allocation and bounded estimation error to deal with position measurement noises at the limits of the field of view, and intermittent occlusion in dynamic environments. Moreover, the aforementioned motion model, uses a learning-based prediction model for input estimation based on a moving buffer of the robot position. Several experiments with occlusion and intermittent visual disruption/detection confirm effectiveness of the developed framework in re-initializing the estimation process after failure in the visual detection, and handling temporary data loss due to sensor faults or changes in lighting conditions.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/infra_robotics_2.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://www.mdpi.com/2218-6581/11/4/82">
                <papertitle>Infrastructure-Aided Localization and State Estimation for Autonomous Mobile Robots</papertitle>
              </a>
              <br>
              <a href="https://www.fzi.de/team/daniel-floegel/">Daniel Flögel</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Robotics Journal, 2022</em>. 
              <br>
              [
              <a href="https://www.mdpi.com/2218-6581/11/4/82">Paper</a>
               ]
              <p>A slip-aware localization framework is proposed for mobile robots experiencing wheel slip in dynamic environments. The framework fuses infrastructure-aided visual tracking data and proprioceptive sensory data from a skid-steer mobile robot to enhance accuracy and reduce variance of the estimated states. Covariance intersection is used to fuse the pose prediction and the visual thread, such that the updated estimate remains consistent. As confirmed by experiments on a skid-steer mobile robot, the designed localization framework addresses state estimation challenges for indoor/outdoor autonomous mobile robots which experience high-slip, uneven torque distribution at each wheel (by the motion planner), or occlusion when observed by an infrastructure-mounted camera.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gpr_smcs.png" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2101.06901">
                <papertitle>Soft Constrained Autonomous Vehicle Navigation using Gaussian Processes and Instance Segmentation</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=yv016ZMAAAAJ&hl=en">Bruno H. Groenner Barbosa</a>,
              <strong>Neel P. Bhatt</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Submitted to Transactions on Systems, Man, and Cybernetics: Systems (SMCS)</em>. 
              <br>
              [
              <a href="https://arxiv.org/pdf/2101.06901">Paper</a>
               ]
              <p>We present a generic feature-based navigation framework for autonomous vehicles using a soft constrained Particle Filter. After obtaining features of mapped landmarks in instance-based segmented images acquired from a monocular camera, vehicle-to-landmark distances are predicted using Gaussian Process Regression (GPR) models in a mixture of experts approach. Experimental results confirm that the use of image segmentation features improves the vehicle-to-landmark distance prediction notably, and that the proposed soft constrained approach reliably localizes the vehicle even with reduced number of landmarks and noisy observations.
              </p>
            </td>
          </tr>

          <tr onmouseout="nightsight_stop()" onmouseover="nightsight_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mhe_itsc.gif" alt="hpp" style="border-style: none" width="350">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">
                <papertitle>Real-time Pedestrian Localization and State Estimation Using Moving Horizon Estimation</papertitle>
              </a>
              <br>
              <a href="https://ca.linkedin.com/in/ehsan-mohammadbagher">Ehsan Mohammadbagher*</a>,
              <strong>Neel P. Bhatt*</strong>,
              <a href="https://apps.ualberta.ca/directory/person/ehashemi">Ehsan Hashemi</a>,
              <br>
              <em>Intelligent Transportation Systems Conference (ITSC), 2020</em>. 
              <br>
              [
              <a href="https://ieeexplore.ieee.org/abstract/document/9294306">Paper</a>
               ]
              <p>We propose a constrained moving horizon state estimation approach to estimate an object's states in 3D with respect to a global stationary frame including position, velocity, and acceleration that are robust to intermittently noisy or absent sensor measurements utilizing a computationally light-weight fusion of 2D dections and projected LIDAR depth measurements. The performance of the proposed approach is experimentally verified on our dataset featuring urban pedestrian crossings.
              </p>
            </td>
          </tr>
               
        </tbody></table>

      </td>
    </tr>
  </table>
  Template from <a href="https://jonbarron.info/">this website</a>.
</body>

</html>
